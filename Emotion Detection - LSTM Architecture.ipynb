{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Datasets Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This portion of the RAVDESS contains 1440 files: 60 trials per actor x 24 actors = 1440. The RAVDESS contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech emotions includes calm, happy, sad, angry, fearful, surprise, and disgust expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression.\n",
    "\n",
    "File naming convention\n",
    "\n",
    "Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n",
    "\n",
    "Filename identifiers\n",
    "\n",
    "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "- Vocal channel (01 = speech, 02 = song).\n",
    "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "- Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "Filename example: 03-01-06-01-02-01-12.wav\n",
    "\n",
    "1. Audio-only (03)\n",
    "2. Speech (01)\n",
    "3. Fearful (06)\n",
    "4. Normal intensity (01)\n",
    "5. Statement \"dogs\" (02)\n",
    "6. 1st Repetition (01)\n",
    "7. 12th Actor (12)\n",
    "Female, as the actor ID number is even."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Crowd Sourced Emotional Multimodal Actors Dataset (CREMA-D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Context\n",
    "\n",
    "I'm on a journey to create an emotion classifier from audio and the CREMA-D dataset is one of the 4 key datasets that I was lucky to stumble upon. What's interesting is that this dataset is the sheer variety of data which helps train a model that can be generalised across new datasets. Many audio datasets use a limited number of speakers which leads to a lot of information leakage. CREMA-D has many speakers. For this fact, the CREMA-D is a very good dataset to use to ensure the model does not overfit.\n",
    "\n",
    "Content\n",
    "\n",
    "CREMA-D is a data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified). Actors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Surrey Audio-Visual Expressed Emotion (SAVEE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Context\n",
    "\n",
    "I'm on a journey to create an emotion classifier from audio and the SAVEE dataset is one of the 4 key datasets that I was lucky to stumble upon. What's interesting is that this dataset is male only and is of very high quality audio. Because the male only speaker will bring about a slightly imbalance representation, it would be advisable to complement other datasets with more female speakers\n",
    "\n",
    "Content\n",
    "\n",
    "The SAVEE database was recorded from four native English male speakers (identified as DC, JE, JK, KL), postgraduate students and researchers at the University of Surrey aged from 27 to 31 years. Emotion has been described psychologically in discrete categories: anger, disgust, fear, happiness, sadness and surprise. A neutral category is also added to provide recordings of 7 emotion categories.\n",
    "\n",
    "The text material consisted of 15 TIMIT sentences per emotion: 3 common, 2 emotion-specific and 10 generic sentences that were different for each emotion and phonetically-balanced. The 3 common and 2 × 6 = 12 emotion-specific sentences were recorded as neutral to give 30 neutral sentences. This resulted in a total of 120 utterances per speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Toronto emotional speech set (TESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Context\n",
    "\n",
    "I'm on a journey to create an emotion classifier from audio and the TESS dataset is one of the 4 key datasets that I was lucky to stumble upon. What's interesting is that this dataset is female only and is of very high quality audio. Most of the other dataset is skewed towards male speakers and thus brings about a slightly imbalance representation. So because of that, this dataset would serve a very good training dataset for the emotion classifier in terms of generalisation (not overfitting)\n",
    "\n",
    "Content\n",
    "\n",
    "There are a set of 200 target words were spoken in the carrier phrase \"Say the word _' by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are 2800 data points (audio files) in total.\n",
    "\n",
    "The dataset is organised such that each of the two female actor and their emotions are contain within its own folder. And within that, all 200 target words audio file can be found. The format of the audio file is a WAV format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:13.966503Z",
     "start_time": "2022-12-23T07:56:58.540015Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa, librosa.display\n",
    "import IPython.display as ipd\n",
    "import math\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy.fft import rfft , rfftfreq , irfft\n",
    "import soundfile as sf\n",
    "import os\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense,LSTM,Dropout,BatchNormalization\n",
    "from keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = \"../Datasets/\"\n",
    "\n",
    "dataset1_path = dataset_path + \"RAVDESS\" + \"/\"\n",
    "dataset2_path = dataset_path + \"CREMA-D\" + \"/\" + \"AudioWAV/\"\n",
    "dataset3_path = dataset_path + \"SAVEE\" + \"/\" + \"ALL/\" \n",
    "dataset4_path = dataset_path + \"TESS\" + \"/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### RAVDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE: (1440, 2)\n",
      "calm        192\n",
      "surprise    192\n",
      "happy       192\n",
      "fear        192\n",
      "sad         192\n",
      "angry       192\n",
      "disgust     192\n",
      "neutral      96\n",
      "Name: Emotions, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>../Datasets/RAVDESS/Actor_01/03-01-01-01-01-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>../Datasets/RAVDESS/Actor_01/03-01-01-01-01-02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>../Datasets/RAVDESS/Actor_01/03-01-01-01-02-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>../Datasets/RAVDESS/Actor_01/03-01-01-01-02-02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calm</td>\n",
       "      <td>../Datasets/RAVDESS/Actor_01/03-01-02-01-01-01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                               Path\n",
       "0  neutral  ../Datasets/RAVDESS/Actor_01/03-01-01-01-01-01...\n",
       "1  neutral  ../Datasets/RAVDESS/Actor_01/03-01-01-01-01-02...\n",
       "2  neutral  ../Datasets/RAVDESS/Actor_01/03-01-01-01-02-01...\n",
       "3  neutral  ../Datasets/RAVDESS/Actor_01/03-01-01-01-02-02...\n",
       "4     calm  ../Datasets/RAVDESS/Actor_01/03-01-02-01-01-01..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE TAKEN FROM: https://www.kaggle.com/code/shivamburnwal/speech-emotion-recognition/notebook\n",
    "Ravdess = dataset1_path\n",
    "ravdess_directory_list = os.listdir(Ravdess)[:-1]\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "for dir in ravdess_directory_list:\n",
    "    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n",
    "    actor = os.listdir(Ravdess + dir)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('-')\n",
    "        # third part in each file represents the emotion associated to that file.\n",
    "        file_emotion.append(int(part[2]))\n",
    "        file_path.append(Ravdess + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# changing integers to actual emotions.\n",
    "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "print(\"SHAPE:\" , Ravdess_df.shape)\n",
    "print(Ravdess_df[\"Emotions\"].value_counts())\n",
    "Ravdess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Ravdess_df.to_csv(\"Final Dataframe.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### CREMA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/CREMA-D/AudioWAV/1001_DFA_ANG_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disgust</td>\n",
       "      <td>Datasets/CREMA-D/AudioWAV/1001_DFA_DIS_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>Datasets/CREMA-D/AudioWAV/1001_DFA_FEA_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>Datasets/CREMA-D/AudioWAV/1001_DFA_HAP_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/CREMA-D/AudioWAV/1001_DFA_NEU_XX.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                           Path\n",
       "0    angry  Datasets/CREMA-D/AudioWAV/1001_DFA_ANG_XX.wav\n",
       "1  disgust  Datasets/CREMA-D/AudioWAV/1001_DFA_DIS_XX.wav\n",
       "2     fear  Datasets/CREMA-D/AudioWAV/1001_DFA_FEA_XX.wav\n",
       "3    happy  Datasets/CREMA-D/AudioWAV/1001_DFA_HAP_XX.wav\n",
       "4  neutral  Datasets/CREMA-D/AudioWAV/1001_DFA_NEU_XX.wav"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crema = dataset2_path\n",
    "crema_directory_list = os.listdir(Crema)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in crema_directory_list:\n",
    "    # storing file paths\n",
    "    file_path.append(Crema + file)\n",
    "    # storing file emotions\n",
    "    part=file.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Crema_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/SAVEE/ALL/DC_a01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/SAVEE/ALL/DC_a02.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/SAVEE/ALL/DC_a03.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/SAVEE/ALL/DC_a04.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/SAVEE/ALL/DC_a05.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                           Path\n",
       "0    angry  Datasets/SAVEE/ALL/DC_a01.wav\n",
       "1    angry  Datasets/SAVEE/ALL/DC_a02.wav\n",
       "2    angry  Datasets/SAVEE/ALL/DC_a03.wav\n",
       "3    angry  Datasets/SAVEE/ALL/DC_a04.wav\n",
       "4    angry  Datasets/SAVEE/ALL/DC_a05.wav"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Savee = dataset3_path\n",
    "savee_directory_list = os.listdir(Savee)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in savee_directory_list:\n",
    "    file_path.append(Savee + file)\n",
    "    part = file.split('_')[1]\n",
    "    ele = part[:-6]\n",
    "    if ele=='a':\n",
    "        file_emotion.append('angry')\n",
    "    elif ele=='d':\n",
    "        file_emotion.append('disgust')\n",
    "    elif ele=='f':\n",
    "        file_emotion.append('fear')\n",
    "    elif ele=='h':\n",
    "        file_emotion.append('happy')\n",
    "    elif ele=='n':\n",
    "        file_emotion.append('neutral')\n",
    "    elif ele=='sa':\n",
    "        file_emotion.append('sad')\n",
    "    else:\n",
    "        file_emotion.append('surprise')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Savee_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### TESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/TESS/OAF_angry/OAF_back_angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/TESS/OAF_angry/OAF_bar_angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/TESS/OAF_angry/OAF_base_angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/TESS/OAF_angry/OAF_bath_angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/TESS/OAF_angry/OAF_bean_angry.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                        Path\n",
       "0    angry  Datasets/TESS/OAF_angry/OAF_back_angry.wav\n",
       "1    angry   Datasets/TESS/OAF_angry/OAF_bar_angry.wav\n",
       "2    angry  Datasets/TESS/OAF_angry/OAF_base_angry.wav\n",
       "3    angry  Datasets/TESS/OAF_angry/OAF_bath_angry.wav\n",
       "4    angry  Datasets/TESS/OAF_angry/OAF_bean_angry.wav"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tess = dataset4_path\n",
    "tess_directory_list = os.listdir(Tess)\n",
    "tess_directory_list.remove('TESS Toronto emotional speech set data')\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in tess_directory_list:\n",
    "    directories = os.listdir(Tess + dir)\n",
    "    for file in directories:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('_')[2]\n",
    "        if part=='ps':\n",
    "            file_emotion.append('surprise')\n",
    "        else:\n",
    "            file_emotion.append(part)\n",
    "        file_path.append(Tess + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Tess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-01-01-01-01-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-01-01-01-02-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-01-01-02-01-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-01-01-02-02-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calm</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-02-01-01-01-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>sad</td>\n",
       "      <td>Datasets/TESS/YAF_sad/YAF_witch_sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>sad</td>\n",
       "      <td>Datasets/TESS/YAF_sad/YAF_yearn_sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>sad</td>\n",
       "      <td>Datasets/TESS/YAF_sad/YAF_yes_sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>sad</td>\n",
       "      <td>Datasets/TESS/YAF_sad/YAF_young_sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>sad</td>\n",
       "      <td>Datasets/TESS/YAF_sad/YAF_youth_sad.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12162 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Emotions                                               Path\n",
       "0     neutral  Datasets/RAVDESS/Actor_01/03-01-01-01-01-01-01...\n",
       "1     neutral  Datasets/RAVDESS/Actor_01/03-01-01-01-01-02-01...\n",
       "2     neutral  Datasets/RAVDESS/Actor_01/03-01-01-01-02-01-01...\n",
       "3     neutral  Datasets/RAVDESS/Actor_01/03-01-01-01-02-02-01...\n",
       "4        calm  Datasets/RAVDESS/Actor_01/03-01-02-01-01-01-01...\n",
       "...       ...                                                ...\n",
       "2795      sad            Datasets/TESS/YAF_sad/YAF_witch_sad.wav\n",
       "2796      sad            Datasets/TESS/YAF_sad/YAF_yearn_sad.wav\n",
       "2797      sad              Datasets/TESS/YAF_sad/YAF_yes_sad.wav\n",
       "2798      sad            Datasets/TESS/YAF_sad/YAF_young_sad.wav\n",
       "2799      sad            Datasets/TESS/YAF_sad/YAF_youth_sad.wav\n",
       "\n",
       "[12162 rows x 2 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([Ravdess_df , Crema_df , Savee_df , Tess_df])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"Final Dataframe.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"./mod1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 921\n",
      "Validation: 231\n",
      "Test: 288\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Final Dataframe.csv\")\n",
    "\n",
    "# df = df[(df[\"Emotions\"] == \"angry\") |\n",
    "#         (df[\"Emotions\"] == \"happy\") |\n",
    "#         (df[\"Emotions\"] == \"sad\") |\n",
    "#         (df[\"Emotions\"] == \"fear\")]\n",
    "\n",
    "\n",
    "Y = df[\"Emotions\"]\n",
    "df_train, df_test = train_test_split(df, test_size=0.20, random_state=26 , stratify = Y)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.20, random_state=26)\n",
    "\n",
    "print(\"Train:\" , df_train.shape[0])\n",
    "print(\"Validation:\" , df_val.shape[0])\n",
    "print(\"Test:\" , df_test.shape[0])\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_val.to_csv(\"Validation.csv\")\n",
    "df_test.to_csv(\"Test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_white_noise(signal, noise_percentage_factor):\n",
    "    noise = np.random.normal(0, signal.std(), signal.size)\n",
    "    augmented_signal = signal + noise * noise_percentage_factor\n",
    "    return augmented_signal\n",
    "\n",
    "\n",
    "def time_stretch(signal, time_stretch_rate):\n",
    "    \"\"\"Time stretching implemented with librosa:\n",
    "    https://librosa.org/doc/main/generated/librosa.effects.pitch_shift.html?highlight=pitch%20shift#librosa.effects.pitch_shift\n",
    "    \"\"\"\n",
    "    return librosa.effects.time_stretch(signal, time_stretch_rate)\n",
    "\n",
    "\n",
    "def pitch_shift(signal, sr, num_semitones):\n",
    "    \"\"\"Pitch scaling implemented with librosa:\n",
    "    https://librosa.org/doc/main/generated/librosa.effects.pitch_shift.html?highlight=pitch%20shift#librosa.effects.pitch_shift\n",
    "    \"\"\"\n",
    "    return librosa.effects.pitch_shift(signal, sr=sr, n_steps=num_semitones)\n",
    "\n",
    "\n",
    "def random_gain(signal, min_factor=0.1, max_factor=0.12):\n",
    "    gain_rate = np.random.uniform(min_factor, max_factor)\n",
    "    augmented_signal = signal * gain_rate\n",
    "    return augmented_signal\n",
    "\n",
    "\n",
    "def time_shift_with_gaussian(signal , sr , name ,label , save_path ,avg_time_shift_by=4):\n",
    "    \n",
    "    shift_by = np.random.normal(sr/avg_time_shift_by , sr/4)  #0.25 seconds\n",
    "    \n",
    "    wav_roll = np.roll(signal , int(shift_by))\n",
    "    aug_signal = add_white_noise(wav_roll , 0.25)\n",
    "    \n",
    "    file_name = name[:-4] + \"_TimeShifted_\" + str(round(len(signal)/shift_by , 2)) + \".wav\"\n",
    "    file_category = label\n",
    "    \n",
    "    sf.write(save_path + file_name, aug_signal, sr, 'PCM_24')\n",
    "\n",
    "    return file_name,file_category\n",
    "        \n",
    "def augment_signal(signal , sr , name ,label , save_path = \"Audios - Train/\",pitch_shifts=1 ,pitch_shift_by=4,time_shift_factor=4):\n",
    "\n",
    "    file_names = []\n",
    "    file_emotions = []\n",
    "#     aug_signal = add_white_noise(signal,0.25)\n",
    "#     file_names,file_emotions = time_shift_with_gaussian(signal , sr , name , label , save_path)\n",
    "\n",
    "    # PITCH_SHIFT__+__TIME_INVARIANCE\n",
    "    \n",
    "    i = 0\n",
    "    while(i < pitch_shifts):\n",
    "        pitch_shifted_signal_1 = random_gain(pitch_shift(signal , sr , (i+pitch_shift_by)))\n",
    "        pitch_shifted_signal_2 = random_gain(pitch_shift(signal , sr , -(i+pitch_shift_by)))\n",
    "        \n",
    "        file_name_1 = name[:-4] + \"_PitchShiftRG_H\" + str(i) + \".wav\"\n",
    "        file_name_2 = name[:-4] + \"_PitchShiftRG_L\" + str(i) + \".wav\"\n",
    "        \n",
    "        file_names_pss1,file_emotions_pss1 = time_shift_with_gaussian(pitch_shifted_signal_1 , sr , file_name_1 , label , save_path, time_shift_factor)\n",
    "        file_names_pss2,file_emotions_pss2 = time_shift_with_gaussian(pitch_shifted_signal_2 , sr , file_name_2 , label , save_path, time_shift_factor)\n",
    "        \n",
    "#         sf.write(save_path + file_name_1, pitch_shifted_signal_1, sr, 'PCM_24')\n",
    "#         sf.write(save_path + file_name_2, pitch_shifted_signal_2, sr, 'PCM_24')\n",
    "        \n",
    "        file_names.append(save_path + file_names_pss1)\n",
    "        file_names.append(save_path + file_names_pss2)\n",
    "        file_emotions.append(file_emotions_pss1)\n",
    "        file_emotions.append(file_emotions_pss2)\n",
    "        \n",
    "#         file_names.append(save_path + file_name_1)\n",
    "#         file_names.append(save_path + file_name_2)\n",
    "#         file_emotions.append(label)\n",
    "#         file_emotions.append(label)\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    return file_names,file_emotions\n",
    "        \n",
    "    \n",
    "    \n",
    "def augment_signal_time_invariance(signal , sr , name ,label , save_path = \"Audios - Train/\",avg_time_shift_by=5):\n",
    "    new_file_names = []\n",
    "    new_file_categories = []\n",
    "    \n",
    "    i=5\n",
    "    while (i>0):\n",
    "        \n",
    "        file_name,file_category = time_shift_with_gaussian(signal , sr , name , label , save_path, avg_time_shift_by)\n",
    "        \n",
    "        new_file_names.append(save_path + file_name)\n",
    "        new_file_categories.append(label)\n",
    "        \n",
    "        i-=1\n",
    "        \n",
    "    return new_file_names, new_file_categories\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Training-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 921/921 [05:18<00:00,  2.89it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = \"Audios - Train 1/\"\n",
    "new_file_paths = []\n",
    "new_labels = []\n",
    "for i,label in enumerate(tqdm(df_train[\"Emotions\"])):\n",
    "    \n",
    "    path = df_train.iloc[i][\"Path\"]\n",
    "    signal,sr = librosa.load(path)\n",
    "    \n",
    "    name = path[path.rfind(\"/\")+1:]\n",
    "    sf.write(save_path + name, signal, sr, 'PCM_24')\n",
    "    new_file_paths.append(save_path + name)\n",
    "    new_labels.append(label)\n",
    "    \n",
    "    file_names , file_emotions = augment_signal(signal , sr ,\"Aug_\" + name , label , save_path)\n",
    "    \n",
    "    new_file_paths.extend(file_names)\n",
    "    new_labels.extend(file_emotions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame()\n",
    "df_temp[\"Emotions\"] = new_labels\n",
    "df_temp[\"Path\"] = new_file_paths\n",
    "df_temp.to_csv(\"Train-1.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Training-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 921/921 [04:54<00:00,  3.13it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = \"Audios - Train 2/\"\n",
    "\n",
    "new_file_paths = []\n",
    "new_labels = []\n",
    "for i,label in enumerate(tqdm(df_train[\"Emotions\"])):\n",
    "    \n",
    "    path = df_train.iloc[i][\"Path\"]\n",
    "    signal,sr = librosa.load(path)\n",
    "    \n",
    "    name = path[path.rfind(\"/\")+1:]\n",
    "    sf.write(save_path + name, signal, sr, 'PCM_24')\n",
    "    new_file_paths.append(save_path + name)\n",
    "    new_labels.append(label)\n",
    "    \n",
    "    file_names , file_emotions = augment_signal(signal , sr ,\"Aug_\" + name , label , save_path,pitch_shift_by=2,time_shift_factor=2)\n",
    "    \n",
    "    new_file_paths.extend(file_names)\n",
    "    new_labels.extend(file_emotions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame()\n",
    "df_temp[\"Emotions\"] = new_labels\n",
    "df_temp[\"Path\"] = new_file_paths\n",
    "df_temp.to_csv(\"Train-2.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Training-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4922/4922 [05:54<00:00, 13.89it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = \"Audios - Train 3/\"\n",
    "# Emotion Detection Training Data/\n",
    "\n",
    "new_file_paths = []\n",
    "new_labels = []\n",
    "for i,label in enumerate(tqdm(df_train[\"Emotions\"])):\n",
    "    \n",
    "    path = df_train.iloc[i][\"Path\"]\n",
    "    signal,sr = librosa.load(path)\n",
    "    \n",
    "    name = path[path.rfind(\"/\")+1:]\n",
    "    sf.write(save_path + name, signal, sr, 'PCM_24')\n",
    "    new_file_paths.append(save_path + name)\n",
    "    new_labels.append(label)\n",
    "    \n",
    "    file_names , file_emotions = augment_signal_time_invariance(signal , sr ,\"Aug\" + name , label , save_path ,avg_time_shift_by=5)\n",
    "    \n",
    "    new_file_paths.extend(file_names)\n",
    "    new_labels.extend(file_emotions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame()\n",
    "df_temp[\"Emotions\"] = new_labels\n",
    "df_temp[\"Path\"] = new_file_paths\n",
    "df_temp.to_csv(\"Train-3.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_val.to_csv(\"Validation.csv\" , index = False)\n",
    "df_test.to_csv(\"Test.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Feature Extraction Using MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:44:55.850388Z",
     "start_time": "2022-12-23T06:44:55.757960Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train_1 = pd.read_csv(\"Train-1.csv\")\n",
    "df_train_2 = pd.read_csv(\"Train-2.csv\")\n",
    "# df_train_3 = pd.read_csv(\"Train-3.csv\")\n",
    "df_train = pd.concat([df_train_1 , df_train_2] , axis = 0)\n",
    "df_val = pd.read_csv(\"Validation.csv\")\n",
    "df_test = pd.read_csv(\"Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:44:57.524788Z",
     "start_time": "2022-12-23T06:44:57.506838Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTRAIN\n",
      "disgust     768\n",
      "calm        744\n",
      "happy       738\n",
      "angry       738\n",
      "fear        732\n",
      "surprise    720\n",
      "sad         714\n",
      "neutral     372\n",
      "Name: Emotions, dtype: int64\n",
      "\n",
      "\n",
      "\tVALIDATION\n",
      "surprise    34\n",
      "sad         34\n",
      "fear        32\n",
      "happy       31\n",
      "angry       30\n",
      "calm        29\n",
      "disgust     26\n",
      "neutral     15\n",
      "Name: Emotions, dtype: int64\n",
      "\n",
      "\n",
      "\tTEST\n",
      "calm        39\n",
      "angry       39\n",
      "sad         39\n",
      "happy       38\n",
      "disgust     38\n",
      "surprise    38\n",
      "fear        38\n",
      "neutral     19\n",
      "Name: Emotions, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\tTRAIN\")\n",
    "print(df_train[\"Emotions\"].value_counts())\n",
    "print(\"\\n\\n\\tVALIDATION\")\n",
    "print(df_val[\"Emotions\"].value_counts())\n",
    "print(\"\\n\\n\\tTEST\")\n",
    "print(df_test[\"Emotions\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max duration at: 18\n",
      "Max duration: 5.2719727891156465\n"
     ]
    }
   ],
   "source": [
    "# _____________________________________\n",
    "# FOR CHECKING MAXIMUM CLIP SIZE\n",
    "# ______________________________________\n",
    "\n",
    "\n",
    "duration = []\n",
    "i = 0\n",
    "while (i < df_train.shape[0]):\n",
    "    \n",
    "    path = df_train.iloc[i][\"Path\"]\n",
    "    clip,sr = librosa.load(path)\n",
    "#     print(\"i =\",i,\"\\tsr = \",sr)\n",
    "    duration.append(len(clip)/sr)\n",
    "#     i+=9\n",
    "    i+=1\n",
    "    print(i,end=\"\\r\")    \n",
    "\n",
    "print(\"Max duration at:\", np.argmax(duration))\n",
    "print(\"Max duration:\" , duration[np.argmax(duration)])\n",
    "# print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:59:00.092038Z",
     "start_time": "2022-12-23T06:59:00.079908Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_white_noise(signal, noise_percentage_factor):\n",
    "    noise = np.random.normal(0, signal.std()+0.050, signal.size)\n",
    "    augmented_signal = signal + noise * noise_percentage_factor\n",
    "    return augmented_signal\n",
    "\n",
    "\n",
    "def get_mfcc(audio , sr , num_mfcc=15 , n_fft=2048 , hop_length=512 , plot = False):\n",
    "    mfcc = librosa.feature.mfcc(y=audio , sr=sr , n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    del_mfcc = librosa.feature.delta(mfcc)\n",
    "    del2_mfcc = librosa.feature.delta(mfcc, order = 2)\n",
    "    features = np.concatenate([mfcc , del_mfcc, del2_mfcc])\n",
    "    \n",
    "    \n",
    "    if (plot == True):\n",
    "        librosa.display.specshow(mfcc, sr=sr, hop_length=hop_length)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"MFCC coefficients\")\n",
    "        plt.colorbar()\n",
    "        plt.title(\"MFCCs\")\n",
    "\n",
    "        # show plots\n",
    "        plt.show()\n",
    "        \n",
    "    return features.T\n",
    "\n",
    "def get_audio_features(path , mfcc_coeff = 15 , max_duration = 4*22050 , to_plot = False) :\n",
    "    \n",
    "    clip,sr = librosa.load(path)\n",
    "\n",
    "    if (clip.shape[0] < max_duration):\n",
    "        while(clip.shape[0] < max_duration):\n",
    "            padding = clip[ : max_duration - clip.shape[0]]\n",
    "            clip = np.append(clip , padding)\n",
    "        \n",
    "    elif(clip.shape[0] > max_duration):\n",
    "        clip = clip[:max_duration]\n",
    "        \n",
    "    data = get_mfcc(np.array(clip),sr, num_mfcc = mfcc_coeff , plot = to_plot) \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### For Train-1 and Train-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:28:19.959062Z",
     "start_time": "2022-12-23T06:59:04.730165Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5526/5526 [00:59<00:00, 93.39it/s]\n"
     ]
    }
   ],
   "source": [
    "og_data = []\n",
    "for path in tqdm(df_train[\"Path\"]):\n",
    "    data = get_audio_features(path = path)\n",
    "    og_data.append(data)\n",
    "    \n",
    "X_train = np.array(og_data) \n",
    "np.save(\"Training Data - Extracted Features - MFCC_15\" , X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### For Validation & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:33:47.272191Z",
     "start_time": "2022-12-23T07:31:16.977562Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 231/231 [00:16<00:00, 13.59it/s]\n"
     ]
    }
   ],
   "source": [
    "val_data = []\n",
    "for path in tqdm(df_val[\"Path\"]):\n",
    "    data = get_audio_features(path = path)\n",
    "    val_data.append(data)\n",
    "    \n",
    "X_val = np.array(val_data) \n",
    "np.save(\"Validation Data - Extracted Features - MFCC_15\" , X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:37:02.058965Z",
     "start_time": "2022-12-23T07:33:53.302774Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 288/288 [00:22<00:00, 12.96it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "for path in tqdm(df_test[\"Path\"]):\n",
    "    data = get_audio_features(path = path)\n",
    "    test_data.append(data)\n",
    "    \n",
    "X_test = np.array(test_data) \n",
    "np.save(\"Test Data - Extracted Features - MFCC_15\" , X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### For Train-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 29532/29532 [18:06<00:00, 27.18it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"Train-3.csv\")\n",
    "\n",
    "mfcc_coeff = 25\n",
    "og_data = []\n",
    "for path in tqdm(df_train[\"Path\"]):\n",
    "    data = get_audio_features(path = path , mfcc_coeff=mfcc_coeff)\n",
    "    og_data.append(data)\n",
    "    \n",
    "X_train = np.array(og_data) \n",
    "np.save(\"Training Data3 - Extracted Features - MFCC_25\" , X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Featurization using Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Training LSTM-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:15.302031Z",
     "start_time": "2022-12-23T07:57:03.430Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat([pd.read_csv(\"Train-1.csv\") , pd.read_csv(\"Train-2.csv\")] , axis = 0)\n",
    "df_val = pd.read_csv(\"Validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:15.308015Z",
     "start_time": "2022-12-23T07:57:03.730Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load(\"Training Data - Extracted Features - MFCC_15.npy\")\n",
    "y_train = df_train[\"Emotions\"] \n",
    "\n",
    "X_val = np.load(\"Validation Data - Extracted Features - MFCC_15.npy\")\n",
    "y_val = df_val[\"Emotions\"]\n",
    "\n",
    "one_hot_encoder = preprocessing.LabelBinarizer()\n",
    "one_hot_encoder.fit(df_train[\"Emotions\"].unique())\n",
    "\n",
    "y_train = one_hot_encoder.transform(y_train)\n",
    "y_val = one_hot_encoder.transform(y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:15.312661Z",
     "start_time": "2022-12-23T07:57:04.655Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (59064, 303, 45)\n",
      "Val: (1231, 303, 45)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\" , X_train.shape)\n",
    "print(\"Val:\" , X_val.shape)\n",
    "# print(\"Test:\" , X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:15.316650Z",
     "start_time": "2022-12-23T07:57:05.005Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes:\" , len(one_hot_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:15.322166Z",
     "start_time": "2022-12-23T07:57:05.380Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 45)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:15.326154Z",
     "start_time": "2022-12-23T07:57:06.167Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# import tensorflow.keras.backend as K \n",
    "# K._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:40:00.157101Z",
     "start_time": "2022-12-23T07:39:58.422850Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 303, 50)           19200     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 25)                7600      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                1300      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                1530      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                620       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 20)               80        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8)                 88        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,628\n",
      "Trainable params: 30,588\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = keras.Sequential()\n",
    "model.add(LSTM(50, kernel_regularizer = regularizers.L1(0.01) ,input_shape=input_shape , return_sequences=True))\n",
    "model.add(LSTM(25 , kernel_regularizer = regularizers.L1(0.01)))\n",
    "model.add(Dense(50, kernel_regularizer = regularizers.L1(0.01) , activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(30, kernel_regularizer = regularizers.L1(0.01) , activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:40:00.867972Z",
     "start_time": "2022-12-23T07:40:00.848941Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# precision = tf.keras.metrics.Precision()\n",
    "# recall = tf.keras.metrics.Recall()\n",
    "model.compile(optimizer='adam',loss='CategoricalCrossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:40:01.870510Z",
     "start_time": "2022-12-23T07:40:01.860494Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gpu_device = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# tf.config.experimental.set_memory_growth(gpu_device[0], True)\n",
    "# # gpu_device[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:52:05.208383Z",
     "start_time": "2022-12-23T07:49:37.334094Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 1.3659 - accuracy: 0.4575 - val_loss: 1.6129 - val_accuracy: 0.4026\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.3484 - accuracy: 0.4546 - val_loss: 1.5633 - val_accuracy: 0.4156\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 1.3270 - accuracy: 0.4586 - val_loss: 1.5768 - val_accuracy: 0.4026\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.3442 - accuracy: 0.4533 - val_loss: 1.6293 - val_accuracy: 0.3593\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.3366 - accuracy: 0.4589 - val_loss: 1.6809 - val_accuracy: 0.3810\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.3236 - accuracy: 0.4692 - val_loss: 1.6743 - val_accuracy: 0.4069\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.3024 - accuracy: 0.4741 - val_loss: 1.6024 - val_accuracy: 0.4026\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.2966 - accuracy: 0.4741 - val_loss: 1.5898 - val_accuracy: 0.4069\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2879 - accuracy: 0.4759 - val_loss: 1.6636 - val_accuracy: 0.3810\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.3059 - accuracy: 0.4662 - val_loss: 1.6677 - val_accuracy: 0.4242\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 1.3023 - accuracy: 0.4736 - val_loss: 1.6916 - val_accuracy: 0.3983\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.3032 - accuracy: 0.4788 - val_loss: 1.6585 - val_accuracy: 0.3853\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.3361 - accuracy: 0.4596 - val_loss: 1.8269 - val_accuracy: 0.3593\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2987 - accuracy: 0.4796 - val_loss: 1.6917 - val_accuracy: 0.3939\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1.2653 - accuracy: 0.4812 - val_loss: 1.6823 - val_accuracy: 0.3896\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.2633 - accuracy: 0.4910 - val_loss: 1.6972 - val_accuracy: 0.4069\n",
      "Epoch 17/50\n",
      " 6/12 [==============>...............] - ETA: 0s - loss: 1.2697 - accuracy: 0.4780"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-6541ca714fff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1570\u001b[1;33m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1571\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    468\u001b[0m         \"\"\"\n\u001b[0;32m    469\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"end\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"end\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             raise ValueError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m             \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1158\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[0mio_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_msg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_break\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\io_utils.py\u001b[0m in \u001b[0;36mprint_msg\u001b[1;34m(message, line_break)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=500, validation_data=(X_val, y_val), shuffle=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:54:42.764711Z",
     "start_time": "2022-12-23T07:54:35.320279Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_LSTM_v3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_LSTM_v3\\assets\n"
     ]
    }
   ],
   "source": [
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(physical_devices)\n",
    "model.save(\"mod1_Emotion_Detector_LSTM_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Training CNN-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:55:04.926689Z",
     "start_time": "2022-12-23T07:55:04.519975Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 303, 128)          28928     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 75, 128)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 75, 128)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 75, 64)            41024     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 37, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 37, 64)            0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2368)              0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2368)             9472      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 25)                59225     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 104       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,753\n",
      "Trainable params: 134,017\n",
      "Non-trainable params: 4,736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Conv1D(128, 5,padding='same',input_shape=input_shape, activation = \"relu\"))\n",
    "model.add(layers.MaxPooling1D(pool_size=(4)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv1D(64, 5,padding='same' , activation = \"relu\"))\n",
    "model.add(layers.MaxPooling1D(pool_size=(2)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(25,activation = \"relu\"))\n",
    "model.add(layers.Dense(4,activation = \"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:56:42.161869Z",
     "start_time": "2022-12-23T07:56:31.733389Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50/50 [==============================] - 5s 89ms/step - loss: 1.1889 - accuracy: 0.6392 - val_loss: 1.2501 - val_accuracy: 0.6637\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.8079 - accuracy: 0.6731 - val_loss: 1.0800 - val_accuracy: 0.6791\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.7147 - accuracy: 0.6956 - val_loss: 0.9499 - val_accuracy: 0.7002\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.6562 - accuracy: 0.7236 - val_loss: 0.9539 - val_accuracy: 0.7051\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.6113 - accuracy: 0.7368 - val_loss: 0.9406 - val_accuracy: 0.7051\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.5984 - accuracy: 0.7442 - val_loss: 0.9499 - val_accuracy: 0.7165\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.5653 - accuracy: 0.7612 - val_loss: 0.9277 - val_accuracy: 0.7149\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.5345 - accuracy: 0.7746 - val_loss: 0.9733 - val_accuracy: 0.7206\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.5130 - accuracy: 0.7816 - val_loss: 0.8723 - val_accuracy: 0.7287\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.5000 - accuracy: 0.7882 - val_loss: 0.9269 - val_accuracy: 0.7157\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - 4s 82ms/step - loss: 0.4772 - accuracy: 0.7993 - val_loss: 1.0086 - val_accuracy: 0.6970\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.4594 - accuracy: 0.8059 - val_loss: 0.9530 - val_accuracy: 0.7124\n",
      "Epoch 13/20\n",
      "50/50 [==============================] - 4s 82ms/step - loss: 0.4445 - accuracy: 0.8112 - val_loss: 1.4752 - val_accuracy: 0.7132\n",
      "Epoch 14/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.4453 - accuracy: 0.8127 - val_loss: 0.9750 - val_accuracy: 0.7335\n",
      "Epoch 15/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.4282 - accuracy: 0.8220 - val_loss: 1.0300 - val_accuracy: 0.7254\n",
      "Epoch 16/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.4099 - accuracy: 0.8276 - val_loss: 1.0904 - val_accuracy: 0.7011\n",
      "Epoch 17/20\n",
      "50/50 [==============================] - 4s 82ms/step - loss: 0.3959 - accuracy: 0.8335 - val_loss: 1.0259 - val_accuracy: 0.7116\n",
      "Epoch 18/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.4007 - accuracy: 0.8329 - val_loss: 1.0664 - val_accuracy: 0.7271\n",
      "Epoch 19/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.3844 - accuracy: 0.8414 - val_loss: 1.0826 - val_accuracy: 0.7157\n",
      "Epoch 20/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.3623 - accuracy: 0.8500 - val_loss: 1.1300 - val_accuracy: 0.7092\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='CategoricalCrossentropy',metrics=['accuracy'])\n",
    "history = model.fit(X_train[:25000], y_train[:25000], epochs=20, batch_size=500, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30/30 [==============================] - 3s 86ms/step - loss: 0.8587 - accuracy: 0.7505 - val_loss: 2.1574 - val_accuracy: 0.6954\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.6449 - accuracy: 0.7457 - val_loss: 1.9185 - val_accuracy: 0.6905\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.5276 - accuracy: 0.7893 - val_loss: 1.5135 - val_accuracy: 0.7035\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.4605 - accuracy: 0.8113 - val_loss: 1.6803 - val_accuracy: 0.7067\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.4109 - accuracy: 0.8379 - val_loss: 1.7433 - val_accuracy: 0.7051\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.3641 - accuracy: 0.8521 - val_loss: 1.4806 - val_accuracy: 0.7084\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.3359 - accuracy: 0.8654 - val_loss: 1.4028 - val_accuracy: 0.7067\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.3032 - accuracy: 0.8787 - val_loss: 1.4364 - val_accuracy: 0.7132\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.2806 - accuracy: 0.8877 - val_loss: 1.4858 - val_accuracy: 0.7157\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.2600 - accuracy: 0.8951 - val_loss: 1.5308 - val_accuracy: 0.7076\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.2479 - accuracy: 0.9020 - val_loss: 1.9866 - val_accuracy: 0.6946\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.2402 - accuracy: 0.9045 - val_loss: 1.5557 - val_accuracy: 0.7051\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 3s 85ms/step - loss: 0.2288 - accuracy: 0.9085 - val_loss: 1.6291 - val_accuracy: 0.6929\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.2183 - accuracy: 0.9135 - val_loss: 1.5691 - val_accuracy: 0.6970\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.2024 - accuracy: 0.9190 - val_loss: 1.6961 - val_accuracy: 0.6937\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1941 - accuracy: 0.9254 - val_loss: 1.6378 - val_accuracy: 0.6937\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1879 - accuracy: 0.9273 - val_loss: 1.7410 - val_accuracy: 0.6767\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1821 - accuracy: 0.9277 - val_loss: 1.5716 - val_accuracy: 0.7076\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1622 - accuracy: 0.9363 - val_loss: 1.6211 - val_accuracy: 0.6978\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 3s 85ms/step - loss: 0.1614 - accuracy: 0.9373 - val_loss: 1.7370 - val_accuracy: 0.6824\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1586 - accuracy: 0.9383 - val_loss: 1.4888 - val_accuracy: 0.7149\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 3s 85ms/step - loss: 0.1493 - accuracy: 0.9403 - val_loss: 1.4741 - val_accuracy: 0.7271\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1468 - accuracy: 0.9434 - val_loss: 1.6376 - val_accuracy: 0.7173\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1354 - accuracy: 0.9484 - val_loss: 1.6138 - val_accuracy: 0.7238\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1409 - accuracy: 0.9451 - val_loss: 1.6768 - val_accuracy: 0.7027\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1295 - accuracy: 0.9491 - val_loss: 1.6224 - val_accuracy: 0.7279\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1279 - accuracy: 0.9503 - val_loss: 1.5992 - val_accuracy: 0.7165\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1238 - accuracy: 0.9535 - val_loss: 1.7173 - val_accuracy: 0.7043\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.1164 - accuracy: 0.9553 - val_loss: 1.7341 - val_accuracy: 0.7132\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 3s 83ms/step - loss: 0.1151 - accuracy: 0.9531 - val_loss: 1.6346 - val_accuracy: 0.7206\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.1108 - accuracy: 0.9601 - val_loss: 1.7353 - val_accuracy: 0.6978\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.1042 - accuracy: 0.9605 - val_loss: 1.6827 - val_accuracy: 0.7141\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0993 - accuracy: 0.9626 - val_loss: 1.7809 - val_accuracy: 0.7206\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.1003 - accuracy: 0.9620 - val_loss: 1.8941 - val_accuracy: 0.7100\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 3s 83ms/step - loss: 0.1002 - accuracy: 0.9609 - val_loss: 1.7354 - val_accuracy: 0.7157\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0986 - accuracy: 0.9633 - val_loss: 1.8778 - val_accuracy: 0.7027\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0909 - accuracy: 0.9667 - val_loss: 1.7409 - val_accuracy: 0.7173\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0923 - accuracy: 0.9633 - val_loss: 1.8208 - val_accuracy: 0.7092\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0883 - accuracy: 0.9675 - val_loss: 1.7868 - val_accuracy: 0.7157\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.0887 - accuracy: 0.9670 - val_loss: 1.7615 - val_accuracy: 0.7165\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0864 - accuracy: 0.9679 - val_loss: 1.8060 - val_accuracy: 0.7238\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0878 - accuracy: 0.9675 - val_loss: 1.9459 - val_accuracy: 0.7116\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0924 - accuracy: 0.9654 - val_loss: 1.8452 - val_accuracy: 0.7076\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0895 - accuracy: 0.9677 - val_loss: 1.8858 - val_accuracy: 0.7076\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 3s 83ms/step - loss: 0.0871 - accuracy: 0.9677 - val_loss: 1.8547 - val_accuracy: 0.7189\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0865 - accuracy: 0.9679 - val_loss: 1.8375 - val_accuracy: 0.7141\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0852 - accuracy: 0.9684 - val_loss: 1.7969 - val_accuracy: 0.7157\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 2s 84ms/step - loss: 0.0846 - accuracy: 0.9695 - val_loss: 1.8068 - val_accuracy: 0.7352\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.0764 - accuracy: 0.9712 - val_loss: 1.7578 - val_accuracy: 0.7214\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 3s 86ms/step - loss: 0.0699 - accuracy: 0.9757 - val_loss: 1.7248 - val_accuracy: 0.7197\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train[25000:40000], y_train[25000:40000], epochs=50, batch_size=500, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 1.0784 - accuracy: 0.7336 - val_loss: 2.0320 - val_accuracy: 0.6409\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.6159 - accuracy: 0.7681 - val_loss: 2.0339 - val_accuracy: 0.6807\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.4659 - accuracy: 0.8132 - val_loss: 1.5479 - val_accuracy: 0.7067\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.3655 - accuracy: 0.8525 - val_loss: 1.4302 - val_accuracy: 0.7181\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.3072 - accuracy: 0.8756 - val_loss: 1.5020 - val_accuracy: 0.7076\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.2814 - accuracy: 0.8867 - val_loss: 1.4955 - val_accuracy: 0.7132\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.2377 - accuracy: 0.9081 - val_loss: 1.5335 - val_accuracy: 0.7197\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1999 - accuracy: 0.9203 - val_loss: 1.4661 - val_accuracy: 0.7197\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.1894 - accuracy: 0.9224 - val_loss: 1.4922 - val_accuracy: 0.7189\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1791 - accuracy: 0.9297 - val_loss: 1.5681 - val_accuracy: 0.7214\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1567 - accuracy: 0.9412 - val_loss: 1.5554 - val_accuracy: 0.7222\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1403 - accuracy: 0.9474 - val_loss: 1.5225 - val_accuracy: 0.7319\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1331 - accuracy: 0.9501 - val_loss: 1.5395 - val_accuracy: 0.7173\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.1191 - accuracy: 0.9570 - val_loss: 1.6083 - val_accuracy: 0.7197\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1151 - accuracy: 0.9577 - val_loss: 1.5858 - val_accuracy: 0.7157\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1019 - accuracy: 0.9621 - val_loss: 1.6654 - val_accuracy: 0.7157\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0942 - accuracy: 0.9661 - val_loss: 1.6702 - val_accuracy: 0.7189\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0871 - accuracy: 0.9679 - val_loss: 1.7855 - val_accuracy: 0.7197\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0888 - accuracy: 0.9674 - val_loss: 1.6527 - val_accuracy: 0.7181\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0788 - accuracy: 0.9709 - val_loss: 1.6360 - val_accuracy: 0.7271\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0715 - accuracy: 0.9744 - val_loss: 1.6298 - val_accuracy: 0.7149\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0672 - accuracy: 0.9750 - val_loss: 1.7443 - val_accuracy: 0.7116\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 0.0680 - accuracy: 0.9759 - val_loss: 1.7128 - val_accuracy: 0.7295\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.0673 - accuracy: 0.9756 - val_loss: 1.7965 - val_accuracy: 0.7165\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.0613 - accuracy: 0.9793 - val_loss: 1.8117 - val_accuracy: 0.7181\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0538 - accuracy: 0.9815 - val_loss: 1.7466 - val_accuracy: 0.7271\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0512 - accuracy: 0.9834 - val_loss: 1.7429 - val_accuracy: 0.7262\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0476 - accuracy: 0.9833 - val_loss: 1.7563 - val_accuracy: 0.7230\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0509 - accuracy: 0.9822 - val_loss: 1.7158 - val_accuracy: 0.7084\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0467 - accuracy: 0.9836 - val_loss: 1.7793 - val_accuracy: 0.7051\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0462 - accuracy: 0.9842 - val_loss: 1.8769 - val_accuracy: 0.7165\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0419 - accuracy: 0.9863 - val_loss: 1.7766 - val_accuracy: 0.7230\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0421 - accuracy: 0.9854 - val_loss: 1.8544 - val_accuracy: 0.7165\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0394 - accuracy: 0.9858 - val_loss: 1.8926 - val_accuracy: 0.7141\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0382 - accuracy: 0.9885 - val_loss: 1.9477 - val_accuracy: 0.7246\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0407 - accuracy: 0.9862 - val_loss: 2.0595 - val_accuracy: 0.6913\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0376 - accuracy: 0.9873 - val_loss: 1.8869 - val_accuracy: 0.7100\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.0324 - accuracy: 0.9891 - val_loss: 2.0132 - val_accuracy: 0.7222\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0316 - accuracy: 0.9903 - val_loss: 1.9516 - val_accuracy: 0.7254\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0290 - accuracy: 0.9905 - val_loss: 2.1979 - val_accuracy: 0.6864\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0316 - accuracy: 0.9911 - val_loss: 2.0947 - val_accuracy: 0.7084\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0359 - accuracy: 0.9885 - val_loss: 1.9569 - val_accuracy: 0.7108\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0305 - accuracy: 0.9901 - val_loss: 1.9039 - val_accuracy: 0.7059\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0289 - accuracy: 0.9906 - val_loss: 1.9865 - val_accuracy: 0.7051\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 0.0276 - accuracy: 0.9907 - val_loss: 1.8645 - val_accuracy: 0.7165\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0255 - accuracy: 0.9920 - val_loss: 1.9246 - val_accuracy: 0.7206\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0254 - accuracy: 0.9922 - val_loss: 1.9621 - val_accuracy: 0.7230\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0255 - accuracy: 0.9923 - val_loss: 1.9370 - val_accuracy: 0.7165\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 1.9236 - val_accuracy: 0.7238\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 1.9508 - val_accuracy: 0.7254\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train[40000:50000], y_train[40000:50000], epochs=50, batch_size=500, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "91/91 [==============================] - 3s 25ms/step - loss: 1.2314 - accuracy: 0.5441 - val_loss: 1.8593 - val_accuracy: 0.4679\n",
      "Epoch 2/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.8699 - accuracy: 0.6134 - val_loss: 1.0803 - val_accuracy: 0.5727\n",
      "Epoch 3/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.7555 - accuracy: 0.6677 - val_loss: 1.0424 - val_accuracy: 0.6296\n",
      "Epoch 4/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.6759 - accuracy: 0.7020 - val_loss: 0.9304 - val_accuracy: 0.6214\n",
      "Epoch 5/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.6234 - accuracy: 0.7287 - val_loss: 1.1106 - val_accuracy: 0.6288\n",
      "Epoch 6/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.5711 - accuracy: 0.7549 - val_loss: 1.0507 - val_accuracy: 0.6523\n",
      "Epoch 7/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.5494 - accuracy: 0.7656 - val_loss: 1.2401 - val_accuracy: 0.6328\n",
      "Epoch 8/50\n",
      "91/91 [==============================] - 2s 25ms/step - loss: 0.5118 - accuracy: 0.7839 - val_loss: 1.0551 - val_accuracy: 0.6515\n",
      "Epoch 9/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.4769 - accuracy: 0.7919 - val_loss: 1.0720 - val_accuracy: 0.6645\n",
      "Epoch 10/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.4385 - accuracy: 0.8144 - val_loss: 1.0611 - val_accuracy: 0.6783\n",
      "Epoch 11/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.4391 - accuracy: 0.8141 - val_loss: 1.0619 - val_accuracy: 0.6661\n",
      "Epoch 12/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.3975 - accuracy: 0.8382 - val_loss: 1.1705 - val_accuracy: 0.6515\n",
      "Epoch 13/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.3623 - accuracy: 0.8490 - val_loss: 1.1369 - val_accuracy: 0.6686\n",
      "Epoch 14/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.3408 - accuracy: 0.8551 - val_loss: 1.2933 - val_accuracy: 0.6807\n",
      "Epoch 15/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.3273 - accuracy: 0.8656 - val_loss: 1.3823 - val_accuracy: 0.6596\n",
      "Epoch 16/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.3068 - accuracy: 0.8768 - val_loss: 1.3039 - val_accuracy: 0.6645\n",
      "Epoch 17/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2873 - accuracy: 0.8837 - val_loss: 1.3224 - val_accuracy: 0.6710\n",
      "Epoch 18/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2467 - accuracy: 0.9005 - val_loss: 1.5593 - val_accuracy: 0.6621\n",
      "Epoch 19/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2968 - accuracy: 0.8799 - val_loss: 1.2709 - val_accuracy: 0.6539\n",
      "Epoch 20/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2909 - accuracy: 0.8822 - val_loss: 1.3940 - val_accuracy: 0.6718\n",
      "Epoch 21/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2598 - accuracy: 0.8961 - val_loss: 1.5946 - val_accuracy: 0.6759\n",
      "Epoch 22/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2272 - accuracy: 0.9093 - val_loss: 1.5916 - val_accuracy: 0.6580\n",
      "Epoch 23/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2195 - accuracy: 0.9121 - val_loss: 1.6072 - val_accuracy: 0.6539\n",
      "Epoch 24/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.2061 - accuracy: 0.9155 - val_loss: 1.4869 - val_accuracy: 0.6556\n",
      "Epoch 25/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1816 - accuracy: 0.9302 - val_loss: 1.6620 - val_accuracy: 0.6288\n",
      "Epoch 26/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1822 - accuracy: 0.9301 - val_loss: 2.6836 - val_accuracy: 0.6223\n",
      "Epoch 27/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1756 - accuracy: 0.9314 - val_loss: 1.4288 - val_accuracy: 0.6653\n",
      "Epoch 28/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1796 - accuracy: 0.9298 - val_loss: 1.4627 - val_accuracy: 0.6832\n",
      "Epoch 29/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1796 - accuracy: 0.9283 - val_loss: 1.5028 - val_accuracy: 0.6385\n",
      "Epoch 30/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1693 - accuracy: 0.9369 - val_loss: 1.5386 - val_accuracy: 0.6604\n",
      "Epoch 31/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1683 - accuracy: 0.9337 - val_loss: 1.6212 - val_accuracy: 0.6653\n",
      "Epoch 32/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1558 - accuracy: 0.9394 - val_loss: 1.6964 - val_accuracy: 0.6507\n",
      "Epoch 33/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1317 - accuracy: 0.9494 - val_loss: 1.4553 - val_accuracy: 0.7100\n",
      "Epoch 34/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1191 - accuracy: 0.9551 - val_loss: 1.6110 - val_accuracy: 0.6604\n",
      "Epoch 35/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1150 - accuracy: 0.9579 - val_loss: 1.5848 - val_accuracy: 0.6702\n",
      "Epoch 36/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1344 - accuracy: 0.9492 - val_loss: 1.7528 - val_accuracy: 0.6166\n",
      "Epoch 37/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1380 - accuracy: 0.9458 - val_loss: 2.0539 - val_accuracy: 0.6710\n",
      "Epoch 38/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1121 - accuracy: 0.9587 - val_loss: 1.6485 - val_accuracy: 0.6799\n",
      "Epoch 39/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1064 - accuracy: 0.9603 - val_loss: 1.7074 - val_accuracy: 0.6726\n",
      "Epoch 40/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1028 - accuracy: 0.9639 - val_loss: 1.7471 - val_accuracy: 0.6669\n",
      "Epoch 41/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.0887 - accuracy: 0.9662 - val_loss: 1.6759 - val_accuracy: 0.6872\n",
      "Epoch 42/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.0908 - accuracy: 0.9655 - val_loss: 1.9620 - val_accuracy: 0.6491\n",
      "Epoch 43/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.0805 - accuracy: 0.9692 - val_loss: 1.7884 - val_accuracy: 0.6677\n",
      "Epoch 44/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1112 - accuracy: 0.9583 - val_loss: 2.1661 - val_accuracy: 0.6751\n",
      "Epoch 45/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1053 - accuracy: 0.9600 - val_loss: 1.8954 - val_accuracy: 0.6872\n",
      "Epoch 46/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.0989 - accuracy: 0.9659 - val_loss: 2.0745 - val_accuracy: 0.6669\n",
      "Epoch 47/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1045 - accuracy: 0.9607 - val_loss: 1.9748 - val_accuracy: 0.6710\n",
      "Epoch 48/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1325 - accuracy: 0.9524 - val_loss: 2.0505 - val_accuracy: 0.6361\n",
      "Epoch 49/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.0981 - accuracy: 0.9632 - val_loss: 1.9990 - val_accuracy: 0.6653\n",
      "Epoch 50/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.0859 - accuracy: 0.9689 - val_loss: 2.0199 - val_accuracy: 0.6580\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train[50000:], y_train[50000:], epochs=50, batch_size=100, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_CNN_v3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_CNN_v3\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Emotion_Detector_CNN_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Train-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### LSTM-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"Emotion_Detector_new_v2\")\n",
    "df_train = pd.read_csv(\"Train-3.csv\")\n",
    "df_val = pd.read_csv(\"Validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load(\"Training Data3 - Extracted Features - MFCC_25.npy\")\n",
    "y_train = df_train[\"Emotions\"] \n",
    "\n",
    "X_val = np.load(\"Validation Data - Extracted Features - MFCC_25.npy\")\n",
    "y_val = df_val[\"Emotions\"]\n",
    "\n",
    "one_hot_encoder = preprocessing.LabelBinarizer()\n",
    "one_hot_encoder.fit(df_train[\"Emotions\"].unique())\n",
    "\n",
    "y_train = one_hot_encoder.transform(y_train)\n",
    "y_val = one_hot_encoder.transform(y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (29532, 303, 25)\n",
      "Val: (1231, 303, 25)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\" , X_train.shape)\n",
    "print(\"Val:\" , X_val.shape)\n",
    "# print(\"Test:\" , X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes:\" , len(one_hot_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 25)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 6s 102ms/step - loss: 0.9293 - accuracy: 0.6127 - val_loss: 0.8899 - val_accuracy: 0.6190\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.8208 - accuracy: 0.6490 - val_loss: 0.8614 - val_accuracy: 0.6288\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.7836 - accuracy: 0.6648 - val_loss: 0.8609 - val_accuracy: 0.6255\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.7643 - accuracy: 0.6693 - val_loss: 0.8587 - val_accuracy: 0.6353\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.7480 - accuracy: 0.6788 - val_loss: 0.8384 - val_accuracy: 0.6442\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 6s 102ms/step - loss: 0.7302 - accuracy: 0.6867 - val_loss: 0.8415 - val_accuracy: 0.6418\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.7236 - accuracy: 0.6879 - val_loss: 0.8517 - val_accuracy: 0.6393\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.7105 - accuracy: 0.6939 - val_loss: 0.8373 - val_accuracy: 0.6450\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.7002 - accuracy: 0.6977 - val_loss: 0.8338 - val_accuracy: 0.6426\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6909 - accuracy: 0.7011 - val_loss: 0.8496 - val_accuracy: 0.6393\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6842 - accuracy: 0.7013 - val_loss: 0.8522 - val_accuracy: 0.6450\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 6s 102ms/step - loss: 0.6770 - accuracy: 0.7061 - val_loss: 0.8752 - val_accuracy: 0.6531\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 6s 103ms/step - loss: 0.6691 - accuracy: 0.7086 - val_loss: 0.8769 - val_accuracy: 0.6296\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 6s 102ms/step - loss: 0.6591 - accuracy: 0.7127 - val_loss: 0.8678 - val_accuracy: 0.6385\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6542 - accuracy: 0.7173 - val_loss: 0.8659 - val_accuracy: 0.6344\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 6s 103ms/step - loss: 0.6471 - accuracy: 0.7194 - val_loss: 0.8985 - val_accuracy: 0.6328\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6416 - accuracy: 0.7242 - val_loss: 0.8620 - val_accuracy: 0.6409\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.6361 - accuracy: 0.7275 - val_loss: 0.8601 - val_accuracy: 0.6426\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.6255 - accuracy: 0.7308 - val_loss: 0.8823 - val_accuracy: 0.6361\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6214 - accuracy: 0.7321 - val_loss: 0.8892 - val_accuracy: 0.6328\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6159 - accuracy: 0.7374 - val_loss: 0.8957 - val_accuracy: 0.6328\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6124 - accuracy: 0.7384 - val_loss: 0.9128 - val_accuracy: 0.6361\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.6093 - accuracy: 0.7414 - val_loss: 0.8755 - val_accuracy: 0.6418\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.6011 - accuracy: 0.7441 - val_loss: 0.9140 - val_accuracy: 0.6312\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6022 - accuracy: 0.7444 - val_loss: 0.8837 - val_accuracy: 0.6409\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.5980 - accuracy: 0.7462 - val_loss: 0.9117 - val_accuracy: 0.6320\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5957 - accuracy: 0.7487 - val_loss: 0.8862 - val_accuracy: 0.6344\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5882 - accuracy: 0.7546 - val_loss: 0.8938 - val_accuracy: 0.6401\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 6s 99ms/step - loss: 0.5858 - accuracy: 0.7562 - val_loss: 0.8814 - val_accuracy: 0.6369\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5761 - accuracy: 0.7589 - val_loss: 0.9094 - val_accuracy: 0.6288\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.5887 - accuracy: 0.7558 - val_loss: 0.9060 - val_accuracy: 0.6434\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 6s 99ms/step - loss: 0.6047 - accuracy: 0.7479 - val_loss: 0.8841 - val_accuracy: 0.6515\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.5810 - accuracy: 0.7587 - val_loss: 0.8954 - val_accuracy: 0.6409\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 6s 99ms/step - loss: 0.5711 - accuracy: 0.7630 - val_loss: 0.8888 - val_accuracy: 0.6409\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 6s 99ms/step - loss: 0.5721 - accuracy: 0.7619 - val_loss: 0.8857 - val_accuracy: 0.6483\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5740 - accuracy: 0.7610 - val_loss: 0.8938 - val_accuracy: 0.6491\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 6s 99ms/step - loss: 0.5679 - accuracy: 0.7636 - val_loss: 0.8909 - val_accuracy: 0.6442\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5625 - accuracy: 0.7662 - val_loss: 0.8984 - val_accuracy: 0.6572\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 6s 99ms/step - loss: 0.5578 - accuracy: 0.7692 - val_loss: 0.9208 - val_accuracy: 0.6531\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5579 - accuracy: 0.7683 - val_loss: 0.8935 - val_accuracy: 0.6564\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5736 - accuracy: 0.7632 - val_loss: 0.8819 - val_accuracy: 0.6653\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5632 - accuracy: 0.7650 - val_loss: 0.9125 - val_accuracy: 0.6483\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5569 - accuracy: 0.7677 - val_loss: 0.9203 - val_accuracy: 0.6596\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5567 - accuracy: 0.7669 - val_loss: 0.9167 - val_accuracy: 0.6450\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5493 - accuracy: 0.7708 - val_loss: 0.9073 - val_accuracy: 0.6531\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.5459 - accuracy: 0.7721 - val_loss: 0.8928 - val_accuracy: 0.6466\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5519 - accuracy: 0.7696 - val_loss: 0.9435 - val_accuracy: 0.6483\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5506 - accuracy: 0.7696 - val_loss: 0.9777 - val_accuracy: 0.6393\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5504 - accuracy: 0.7699 - val_loss: 0.9792 - val_accuracy: 0.6491\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.5436 - accuracy: 0.7701 - val_loss: 0.9027 - val_accuracy: 0.6548\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=500, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_new_v2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_new_v2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Emotion_Detector_new_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"Emotion_Detector_new_CNN_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 5s 74ms/step - loss: 0.1719 - accuracy: 0.9345 - val_loss: 1.8509 - val_accuracy: 0.7400\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1574 - accuracy: 0.9402 - val_loss: 1.8037 - val_accuracy: 0.7425\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1505 - accuracy: 0.9427 - val_loss: 1.7518 - val_accuracy: 0.7238\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1606 - accuracy: 0.9394 - val_loss: 2.1389 - val_accuracy: 0.7409\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1509 - accuracy: 0.9436 - val_loss: 2.4488 - val_accuracy: 0.7433\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1516 - accuracy: 0.9429 - val_loss: 2.1439 - val_accuracy: 0.7262\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1472 - accuracy: 0.9451 - val_loss: 2.8443 - val_accuracy: 0.7376\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1707 - accuracy: 0.9361 - val_loss: 3.4290 - val_accuracy: 0.7181\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1447 - accuracy: 0.9462 - val_loss: 3.0728 - val_accuracy: 0.7238\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1443 - accuracy: 0.9449 - val_loss: 4.2604 - val_accuracy: 0.7189\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1346 - accuracy: 0.9497 - val_loss: 3.7890 - val_accuracy: 0.6937\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1355 - accuracy: 0.9490 - val_loss: 1.7121 - val_accuracy: 0.7400\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1310 - accuracy: 0.9513 - val_loss: 1.8253 - val_accuracy: 0.7319\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1342 - accuracy: 0.9484 - val_loss: 1.9104 - val_accuracy: 0.7173\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.1368 - accuracy: 0.9482 - val_loss: 1.8051 - val_accuracy: 0.7189\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.1285 - accuracy: 0.9538 - val_loss: 1.9958 - val_accuracy: 0.7287\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1292 - accuracy: 0.9515 - val_loss: 2.2557 - val_accuracy: 0.7417\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1256 - accuracy: 0.9529 - val_loss: 2.3019 - val_accuracy: 0.7092\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1266 - accuracy: 0.9527 - val_loss: 2.3012 - val_accuracy: 0.7230\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1369 - accuracy: 0.9485 - val_loss: 2.0892 - val_accuracy: 0.7246\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1290 - accuracy: 0.9530 - val_loss: 1.9340 - val_accuracy: 0.7189\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1538 - accuracy: 0.9417 - val_loss: 2.1136 - val_accuracy: 0.7262\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1390 - accuracy: 0.9470 - val_loss: 2.3978 - val_accuracy: 0.7100\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1448 - accuracy: 0.9458 - val_loss: 2.3473 - val_accuracy: 0.7311\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1270 - accuracy: 0.9523 - val_loss: 1.9089 - val_accuracy: 0.7344\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1175 - accuracy: 0.9559 - val_loss: 2.0087 - val_accuracy: 0.7295\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1155 - accuracy: 0.9572 - val_loss: 2.3009 - val_accuracy: 0.7271\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1335 - accuracy: 0.9491 - val_loss: 1.9195 - val_accuracy: 0.7189\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1239 - accuracy: 0.9547 - val_loss: 1.6412 - val_accuracy: 0.7425\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1181 - accuracy: 0.9565 - val_loss: 1.7866 - val_accuracy: 0.7197\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1140 - accuracy: 0.9580 - val_loss: 1.8869 - val_accuracy: 0.7197\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1136 - accuracy: 0.9590 - val_loss: 1.9851 - val_accuracy: 0.7181\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1063 - accuracy: 0.9605 - val_loss: 2.0592 - val_accuracy: 0.7189\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1077 - accuracy: 0.9596 - val_loss: 1.9267 - val_accuracy: 0.7392\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1042 - accuracy: 0.9617 - val_loss: 1.9741 - val_accuracy: 0.7214\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1128 - accuracy: 0.9591 - val_loss: 1.9374 - val_accuracy: 0.7335\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1070 - accuracy: 0.9607 - val_loss: 2.2587 - val_accuracy: 0.7303\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1053 - accuracy: 0.9613 - val_loss: 2.1567 - val_accuracy: 0.7189\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1030 - accuracy: 0.9618 - val_loss: 2.0418 - val_accuracy: 0.7352\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1048 - accuracy: 0.9618 - val_loss: 2.0057 - val_accuracy: 0.7287\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1092 - accuracy: 0.9599 - val_loss: 2.1925 - val_accuracy: 0.7173\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1156 - accuracy: 0.9572 - val_loss: 2.0124 - val_accuracy: 0.7295\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1113 - accuracy: 0.9592 - val_loss: 2.2508 - val_accuracy: 0.7271\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1389 - accuracy: 0.9495 - val_loss: 2.3905 - val_accuracy: 0.7335\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1045 - accuracy: 0.9614 - val_loss: 2.6854 - val_accuracy: 0.7271\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1222 - accuracy: 0.9556 - val_loss: 2.4408 - val_accuracy: 0.7287\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1166 - accuracy: 0.9577 - val_loss: 2.4415 - val_accuracy: 0.7254\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1060 - accuracy: 0.9600 - val_loss: 1.8488 - val_accuracy: 0.7254\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1017 - accuracy: 0.9634 - val_loss: 1.6324 - val_accuracy: 0.7238\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1676 - accuracy: 0.9412 - val_loss: 1.6875 - val_accuracy: 0.7360\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=500, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_new_CNN_v2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_new_CNN_v2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Emotion_Detector_new_CNN_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### LSTM-CNN using Train-1 & Train-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### LSTM-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_train_1 = pd.read_csv(\"Train-1.csv\")\n",
    "# df_train_2 = pd.read_csv(\"Train-2.csv\")\n",
    "df_train = pd.concat([pd.read_csv(\"Train-1.csv\"),\n",
    "                      pd.read_csv(\"Train-2.csv\"),] , axis = 0)\n",
    "df_val = pd.read_csv(\"Validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load(\"Training Data3 - Extracted Features - MFCC_25.npy\")\n",
    "y_train = df_train[\"Emotions\"] \n",
    "\n",
    "X_val = np.load(\"Validation Data - Extracted Features - MFCC_25.npy\")\n",
    "y_val = df_val[\"Emotions\"]\n",
    "\n",
    "one_hot_encoder = preprocessing.LabelBinarizer()\n",
    "one_hot_encoder.fit(df_train[\"Emotions\"].unique())\n",
    "\n",
    "y_train = one_hot_encoder.transform(y_train)\n",
    "y_val = one_hot_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (29532, 303, 25)\n",
      "Val: (1231, 303, 25)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\" , X_train.shape)\n",
    "print(\"Val:\" , X_val.shape)\n",
    "# print(\"Test:\" , X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes:\" , len(one_hot_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 303, 25)           5100      \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 303, 64)           8064      \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 303, 32)           10272     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 303, 32)           0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 25)                5800      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 20)                520       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 20)               80        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                210       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 4)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,090\n",
      "Trainable params: 30,050\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(25 , return_sequences = True , input_shape = input_shape))\n",
    "model.add(layers.Conv1D(64, 5, padding = \"same\" , activation = \"relu\"))\n",
    "model.add(layers.Conv1D(32, 5, padding = \"same\", activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "# model.add(layers.Flatten())\n",
    "model.add(layers.LSTM(25))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# model.build(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30/30 [==============================] - 10s 227ms/step - loss: 1.3903 - accuracy: 0.2490 - val_loss: 1.3857 - val_accuracy: 0.2616\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 6s 196ms/step - loss: 1.3865 - accuracy: 0.2547 - val_loss: 1.3865 - val_accuracy: 0.2218\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 6s 194ms/step - loss: 1.3863 - accuracy: 0.2534 - val_loss: 1.3867 - val_accuracy: 0.2226\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 6s 196ms/step - loss: 1.3861 - accuracy: 0.2573 - val_loss: 1.3870 - val_accuracy: 0.2218\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 6s 195ms/step - loss: 1.3861 - accuracy: 0.2565 - val_loss: 1.3870 - val_accuracy: 0.2218\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6dac35f54ec9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'CategoricalCrossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='CategoricalCrossentropy',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=1000, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save(\"Emotion_Detector_LSTM_CNN_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"Emotion_Detector_LSTM_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"Test.csv\")\n",
    "X_test = np.load(\"Test Data - Extracted Features - MFCC_15.npy\")\n",
    "y_test = df_test[\"Emotions\"]\n",
    "\n",
    "one_hot_encoder = preprocessing.LabelBinarizer()\n",
    "one_hot_encoder.fit(pd.read_csv(\"Train-1.csv\")[\"Emotions\"].unique())\n",
    "y_test = one_hot_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['angry', 'fear', 'happy', 'sad']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(one_hot_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 2s 18ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.77      0.75      0.76       385\n",
      "        fear       0.72      0.33      0.45       385\n",
      "       happy       0.54      0.70      0.61       385\n",
      "         sad       0.66      0.85      0.74       384\n",
      "\n",
      "    accuracy                           0.66      1539\n",
      "   macro avg       0.67      0.66      0.64      1539\n",
      "weighted avg       0.67      0.66      0.64      1539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# X_test.shape = (1,data.shape[0],data.shape[1])\n",
    "from sklearn.metrics import classification_report as cr\n",
    "\n",
    "y_prob = model.predict(X_test) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "y_true = y_test.argmax(axis=-1)\n",
    "print(cr(y_true, y_pred , target_names=one_hot_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 2s 19ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.69      0.83      0.75       385\n",
      "        fear       0.58      0.31      0.40       385\n",
      "       happy       0.62      0.54      0.58       385\n",
      "         sad       0.60      0.85      0.70       384\n",
      "\n",
      "    accuracy                           0.63      1539\n",
      "   macro avg       0.62      0.63      0.61      1539\n",
      "weighted avg       0.62      0.63      0.61      1539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##OLD\n",
    "\n",
    "# X_test.shape = (1,data.shape[0],data.shape[1])\n",
    "from sklearn.metrics import classification_report as cr\n",
    "\n",
    "y_prob = model.predict(X_test) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "y_true = y_test.argmax(axis=-1)\n",
    "print(cr(y_true, y_pred , target_names=one_hot_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"Emotion_Detector_new_CNN_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.75      0.88      0.81       385\n",
      "        fear       0.62      0.64      0.63       385\n",
      "       happy       0.71      0.63      0.67       385\n",
      "         sad       0.76      0.69      0.72       384\n",
      "\n",
      "    accuracy                           0.71      1539\n",
      "   macro avg       0.71      0.71      0.71      1539\n",
      "weighted avg       0.71      0.71      0.71      1539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OLD\n",
    "# __________________________________\n",
    "df_test = pd.read_csv(\"Test.csv\")\n",
    "X_test = np.load(\"Test Data - Extracted Features - MFCC_25.npy\")\n",
    "y_test = df_test[\"Emotions\"] \n",
    "\n",
    "one_hot_encoder = preprocessing.LabelBinarizer()\n",
    "one_hot_encoder.fit(pd.read_csv(\"Train-1.csv\")[\"Emotions\"].unique())\n",
    "y_test = one_hot_encoder.transform(y_test)\n",
    "y_prob = model.predict(X_test) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "y_true = y_test.argmax(axis=-1)\n",
    "print(cr(y_true, y_pred , target_names=one_hot_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"Emotion_Detector_CNN_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.87      0.82      0.84       385\n",
      "        fear       0.58      0.73      0.65       385\n",
      "       happy       0.66      0.72      0.69       385\n",
      "         sad       0.84      0.58      0.68       384\n",
      "\n",
      "    accuracy                           0.71      1539\n",
      "   macro avg       0.74      0.71      0.72      1539\n",
      "weighted avg       0.74      0.71      0.72      1539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"Test.csv\")\n",
    "X_test = np.load(\"Test Data - Extracted Features - MFCC_15.npy\")\n",
    "y_test = df_test[\"Emotions\"] \n",
    "\n",
    "one_hot_encoder = preprocessing.LabelBinarizer()\n",
    "one_hot_encoder.fit(pd.read_csv(\"Train-1.csv\")[\"Emotions\"].unique())\n",
    "y_test = one_hot_encoder.transform(y_test)\n",
    "y_prob = model.predict(X_test) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "y_true = y_test.argmax(axis=-1)\n",
    "print(cr(y_true, y_pred , target_names=one_hot_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
