{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T19:03:26.145057Z",
     "start_time": "2022-12-22T19:03:23.816380Z"
    }
   },
   "outputs": [],
   "source": [
    "!jt -t chesterish -tfs 11 -nfs 13 -ofs 11 -cellw 80% -dfonts -T -kl -N -vim\n",
    "# !jt -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T19:05:46.330289Z",
     "start_time": "2022-12-22T19:05:44.865983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset css and font defaults in:\n",
      "C:\\Users\\aarat\\.jupyter\\custom &\n",
      "C:\\Users\\aarat\\AppData\\Roaming\\jupyter\\nbextensions\n"
     ]
    }
   ],
   "source": [
    "!jt -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T18:41:51.598484Z",
     "start_time": "2022-12-22T18:41:50.142408Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Datasets Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion of the RAVDESS contains 1440 files: 60 trials per actor x 24 actors = 1440. The RAVDESS contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech emotions includes calm, happy, sad, angry, fearful, surprise, and disgust expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression.\n",
    "\n",
    "File naming convention\n",
    "\n",
    "Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n",
    "\n",
    "Filename identifiers\n",
    "\n",
    "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "- Vocal channel (01 = speech, 02 = song).\n",
    "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "- Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "Filename example: 03-01-06-01-02-01-12.wav\n",
    "\n",
    "1. Audio-only (03)\n",
    "2. Speech (01)\n",
    "3. Fearful (06)\n",
    "4. Normal intensity (01)\n",
    "5. Statement \"dogs\" (02)\n",
    "6. 1st Repetition (01)\n",
    "7. 12th Actor (12)\n",
    "Female, as the actor ID number is even."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Crowd Sourced Emotional Multimodal Actors Dataset (CREMA-D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Context\n",
    "\n",
    "I'm on a journey to create an emotion classifier from audio and the CREMA-D dataset is one of the 4 key datasets that I was lucky to stumble upon. What's interesting is that this dataset is the sheer variety of data which helps train a model that can be generalised across new datasets. Many audio datasets use a limited number of speakers which leads to a lot of information leakage. CREMA-D has many speakers. For this fact, the CREMA-D is a very good dataset to use to ensure the model does not overfit.\n",
    "\n",
    "Content\n",
    "\n",
    "CREMA-D is a data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified). Actors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Surrey Audio-Visual Expressed Emotion (SAVEE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Context\n",
    "\n",
    "I'm on a journey to create an emotion classifier from audio and the SAVEE dataset is one of the 4 key datasets that I was lucky to stumble upon. What's interesting is that this dataset is male only and is of very high quality audio. Because the male only speaker will bring about a slightly imbalance representation, it would be advisable to complement other datasets with more female speakers\n",
    "\n",
    "Content\n",
    "\n",
    "The SAVEE database was recorded from four native English male speakers (identified as DC, JE, JK, KL), postgraduate students and researchers at the University of Surrey aged from 27 to 31 years. Emotion has been described psychologically in discrete categories: anger, disgust, fear, happiness, sadness and surprise. A neutral category is also added to provide recordings of 7 emotion categories.\n",
    "\n",
    "The text material consisted of 15 TIMIT sentences per emotion: 3 common, 2 emotion-specific and 10 generic sentences that were different for each emotion and phonetically-balanced. The 3 common and 2 Ã— 6 = 12 emotion-specific sentences were recorded as neutral to give 30 neutral sentences. This resulted in a total of 120 utterances per speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Toronto emotional speech set (TESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Context\n",
    "\n",
    "I'm on a journey to create an emotion classifier from audio and the TESS dataset is one of the 4 key datasets that I was lucky to stumble upon. What's interesting is that this dataset is female only and is of very high quality audio. Most of the other dataset is skewed towards male speakers and thus brings about a slightly imbalance representation. So because of that, this dataset would serve a very good training dataset for the emotion classifier in terms of generalisation (not overfitting)\n",
    "\n",
    "Content\n",
    "\n",
    "There are a set of 200 target words were spoken in the carrier phrase \"Say the word _' by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are 2800 data points (audio files) in total.\n",
    "\n",
    "The dataset is organised such that each of the two female actor and their emotions are contain within its own folder. And within that, all 200 target words audio file can be found. The format of the audio file is a WAV format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:13.966503Z",
     "start_time": "2022-12-23T07:56:58.540015Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa, librosa.display\n",
    "import IPython.display as ipd\n",
    "import math\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy.fft import rfft , rfftfreq , irfft\n",
    "import soundfile as sf\n",
    "import os\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense,LSTM,Dropout,BatchNormalization\n",
    "from keras import layers\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = \"Datasets/\"\n",
    "\n",
    "dataset1_path = dataset_path + \"RAVDESS\" + \"/\"\n",
    "dataset2_path = dataset_path + \"CREMA-D\" + \"/\" + \"AudioWAV/\"\n",
    "dataset3_path = dataset_path + \"SAVEE\" + \"/\" + \"ALL/\" \n",
    "dataset4_path = dataset_path + \"TESS\" + \"/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### RAVDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-01-01-01-01-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-01-01-01-02-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-01-01-02-01-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-01-01-02-02-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calm</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-02-01-01-01-01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                               Path\n",
       "0  neutral  Datasets/RAVDESS/Actor_01/03-01-01-01-01-01-01...\n",
       "1  neutral  Datasets/RAVDESS/Actor_01/03-01-01-01-01-02-01...\n",
       "2  neutral  Datasets/RAVDESS/Actor_01/03-01-01-01-02-01-01...\n",
       "3  neutral  Datasets/RAVDESS/Actor_01/03-01-01-01-02-02-01...\n",
       "4     calm  Datasets/RAVDESS/Actor_01/03-01-02-01-01-01-01..."
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE TAKEN FROM: https://www.kaggle.com/code/shivamburnwal/speech-emotion-recognition/notebook\n",
    "Ravdess = dataset1_path\n",
    "ravdess_directory_list = os.listdir(Ravdess)[:-1]\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "for dir in ravdess_directory_list:\n",
    "    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n",
    "    actor = os.listdir(Ravdess + dir)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('-')\n",
    "        # third part in each file represents the emotion associated to that file.\n",
    "        file_emotion.append(int(part[2]))\n",
    "        file_path.append(Ravdess + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# changing integers to actual emotions.\n",
    "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "Ravdess_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### CREMA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/CREMA-D/AudioWAV/1001_DFA_ANG_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disgust</td>\n",
       "      <td>Datasets/CREMA-D/AudioWAV/1001_DFA_DIS_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>Datasets/CREMA-D/AudioWAV/1001_DFA_FEA_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>Datasets/CREMA-D/AudioWAV/1001_DFA_HAP_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/CREMA-D/AudioWAV/1001_DFA_NEU_XX.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                           Path\n",
       "0    angry  Datasets/CREMA-D/AudioWAV/1001_DFA_ANG_XX.wav\n",
       "1  disgust  Datasets/CREMA-D/AudioWAV/1001_DFA_DIS_XX.wav\n",
       "2     fear  Datasets/CREMA-D/AudioWAV/1001_DFA_FEA_XX.wav\n",
       "3    happy  Datasets/CREMA-D/AudioWAV/1001_DFA_HAP_XX.wav\n",
       "4  neutral  Datasets/CREMA-D/AudioWAV/1001_DFA_NEU_XX.wav"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crema = dataset2_path\n",
    "crema_directory_list = os.listdir(Crema)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in crema_directory_list:\n",
    "    # storing file paths\n",
    "    file_path.append(Crema + file)\n",
    "    # storing file emotions\n",
    "    part=file.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Crema_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/SAVEE/ALL/DC_a01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/SAVEE/ALL/DC_a02.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/SAVEE/ALL/DC_a03.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/SAVEE/ALL/DC_a04.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/SAVEE/ALL/DC_a05.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                           Path\n",
       "0    angry  Datasets/SAVEE/ALL/DC_a01.wav\n",
       "1    angry  Datasets/SAVEE/ALL/DC_a02.wav\n",
       "2    angry  Datasets/SAVEE/ALL/DC_a03.wav\n",
       "3    angry  Datasets/SAVEE/ALL/DC_a04.wav\n",
       "4    angry  Datasets/SAVEE/ALL/DC_a05.wav"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Savee = dataset3_path\n",
    "savee_directory_list = os.listdir(Savee)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in savee_directory_list:\n",
    "    file_path.append(Savee + file)\n",
    "    part = file.split('_')[1]\n",
    "    ele = part[:-6]\n",
    "    if ele=='a':\n",
    "        file_emotion.append('angry')\n",
    "    elif ele=='d':\n",
    "        file_emotion.append('disgust')\n",
    "    elif ele=='f':\n",
    "        file_emotion.append('fear')\n",
    "    elif ele=='h':\n",
    "        file_emotion.append('happy')\n",
    "    elif ele=='n':\n",
    "        file_emotion.append('neutral')\n",
    "    elif ele=='sa':\n",
    "        file_emotion.append('sad')\n",
    "    else:\n",
    "        file_emotion.append('surprise')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Savee_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### TESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/TESS/OAF_angry/OAF_back_angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/TESS/OAF_angry/OAF_bar_angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/TESS/OAF_angry/OAF_base_angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/TESS/OAF_angry/OAF_bath_angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/TESS/OAF_angry/OAF_bean_angry.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                        Path\n",
       "0    angry  Datasets/TESS/OAF_angry/OAF_back_angry.wav\n",
       "1    angry   Datasets/TESS/OAF_angry/OAF_bar_angry.wav\n",
       "2    angry  Datasets/TESS/OAF_angry/OAF_base_angry.wav\n",
       "3    angry  Datasets/TESS/OAF_angry/OAF_bath_angry.wav\n",
       "4    angry  Datasets/TESS/OAF_angry/OAF_bean_angry.wav"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tess = dataset4_path\n",
    "tess_directory_list = os.listdir(Tess)\n",
    "tess_directory_list.remove('TESS Toronto emotional speech set data')\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in tess_directory_list:\n",
    "    directories = os.listdir(Tess + dir)\n",
    "    for file in directories:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('_')[2]\n",
    "        if part=='ps':\n",
    "            file_emotion.append('surprise')\n",
    "        else:\n",
    "            file_emotion.append(part)\n",
    "        file_path.append(Tess + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Tess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-01-01-01-01-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-01-01-01-02-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-01-01-02-01-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-01-01-02-02-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calm</td>\n",
       "      <td>Datasets/RAVDESS/Actor_01/03-01-02-01-01-01-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>sad</td>\n",
       "      <td>Datasets/TESS/YAF_sad/YAF_witch_sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>sad</td>\n",
       "      <td>Datasets/TESS/YAF_sad/YAF_yearn_sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>sad</td>\n",
       "      <td>Datasets/TESS/YAF_sad/YAF_yes_sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>sad</td>\n",
       "      <td>Datasets/TESS/YAF_sad/YAF_young_sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>sad</td>\n",
       "      <td>Datasets/TESS/YAF_sad/YAF_youth_sad.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12162 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Emotions                                               Path\n",
       "0     neutral  Datasets/RAVDESS/Actor_01/03-01-01-01-01-01-01...\n",
       "1     neutral  Datasets/RAVDESS/Actor_01/03-01-01-01-01-02-01...\n",
       "2     neutral  Datasets/RAVDESS/Actor_01/03-01-01-01-02-01-01...\n",
       "3     neutral  Datasets/RAVDESS/Actor_01/03-01-01-01-02-02-01...\n",
       "4        calm  Datasets/RAVDESS/Actor_01/03-01-02-01-01-01-01...\n",
       "...       ...                                                ...\n",
       "2795      sad            Datasets/TESS/YAF_sad/YAF_witch_sad.wav\n",
       "2796      sad            Datasets/TESS/YAF_sad/YAF_yearn_sad.wav\n",
       "2797      sad              Datasets/TESS/YAF_sad/YAF_yes_sad.wav\n",
       "2798      sad            Datasets/TESS/YAF_sad/YAF_young_sad.wav\n",
       "2799      sad            Datasets/TESS/YAF_sad/YAF_youth_sad.wav\n",
       "\n",
       "[12162 rows x 2 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([Ravdess_df , Crema_df , Savee_df , Tess_df])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"Final Dataframe.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 7783\n",
      "Validation: 1946\n",
      "Test: 2433\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Final Dataframe.csv\")\n",
    "\n",
    "# df = df[(df[\"Emotions\"] == \"angry\") |\n",
    "#         (df[\"Emotions\"] == \"happy\") |\n",
    "#         (df[\"Emotions\"] == \"sad\") |\n",
    "#         (df[\"Emotions\"] == \"fear\")]\n",
    "\n",
    "\n",
    "Y = df[\"Emotions\"]\n",
    "df_train, df_test = train_test_split(df, test_size=0.20, random_state=26 , stratify = Y)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.20, random_state=26)\n",
    "\n",
    "print(\"Train:\" , df_train.shape[0])\n",
    "print(\"Validation:\" , df_val.shape[0])\n",
    "print(\"Test:\" , df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9430</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/TESS/OAF_angry/OAF_join_angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11590</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/TESS/YAF_neutral/YAF_death_neutral.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9078</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Datasets/SAVEE/ALL/JE_n17.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>happy</td>\n",
       "      <td>Datasets/RAVDESS/Actor_10/03-01-03-01-02-01-10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3454</th>\n",
       "      <td>disgust</td>\n",
       "      <td>Datasets/CREMA-D/AudioWAV/1025_TIE_DIS_XX.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>happy</td>\n",
       "      <td>Datasets/RAVDESS/Actor_06/03-01-03-02-01-02-06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10803</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/TESS/YAF_angry/YAF_fit_angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10607</th>\n",
       "      <td>sad</td>\n",
       "      <td>Datasets/TESS/OAF_Sad/OAF_gas_sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10919</th>\n",
       "      <td>angry</td>\n",
       "      <td>Datasets/TESS/YAF_angry/YAF_soap_angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7543</th>\n",
       "      <td>fear</td>\n",
       "      <td>Datasets/CREMA-D/AudioWAV/1075_MTI_FEA_XX.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7783 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Emotions                                               Path\n",
       "9430     angry         Datasets/TESS/OAF_angry/OAF_join_angry.wav\n",
       "11590  neutral    Datasets/TESS/YAF_neutral/YAF_death_neutral.wav\n",
       "9078   neutral                      Datasets/SAVEE/ALL/JE_n17.wav\n",
       "554      happy  Datasets/RAVDESS/Actor_10/03-01-03-01-02-01-10...\n",
       "3454   disgust      Datasets/CREMA-D/AudioWAV/1025_TIE_DIS_XX.wav\n",
       "...        ...                                                ...\n",
       "317      happy  Datasets/RAVDESS/Actor_06/03-01-03-02-01-02-06...\n",
       "10803    angry          Datasets/TESS/YAF_angry/YAF_fit_angry.wav\n",
       "10607      sad              Datasets/TESS/OAF_Sad/OAF_gas_sad.wav\n",
       "10919    angry         Datasets/TESS/YAF_angry/YAF_soap_angry.wav\n",
       "7543      fear      Datasets/CREMA-D/AudioWAV/1075_MTI_FEA_XX.wav\n",
       "\n",
       "[7783 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_white_noise(signal, noise_percentage_factor):\n",
    "    noise = np.random.normal(0, signal.std(), signal.size)\n",
    "    augmented_signal = signal + noise * noise_percentage_factor\n",
    "    return augmented_signal\n",
    "\n",
    "\n",
    "def time_stretch(signal, time_stretch_rate):\n",
    "    \"\"\"Time stretching implemented with librosa:\n",
    "    https://librosa.org/doc/main/generated/librosa.effects.pitch_shift.html?highlight=pitch%20shift#librosa.effects.pitch_shift\n",
    "    \"\"\"\n",
    "    return librosa.effects.time_stretch(signal, time_stretch_rate)\n",
    "\n",
    "\n",
    "def pitch_shift(signal, sr, num_semitones):\n",
    "    \"\"\"Pitch scaling implemented with librosa:\n",
    "    https://librosa.org/doc/main/generated/librosa.effects.pitch_shift.html?highlight=pitch%20shift#librosa.effects.pitch_shift\n",
    "    \"\"\"\n",
    "    return librosa.effects.pitch_shift(signal, sr=sr, n_steps=num_semitones)\n",
    "\n",
    "\n",
    "def random_gain(signal, min_factor=0.1, max_factor=0.12):\n",
    "    gain_rate = np.random.uniform(min_factor, max_factor)\n",
    "    augmented_signal = signal * gain_rate\n",
    "    return augmented_signal\n",
    "\n",
    "\n",
    "def time_shift_with_gaussian(signal , sr , name ,label , save_path ,avg_time_shift_by=4):\n",
    "    \n",
    "    shift_by = np.random.normal(sr/avg_time_shift_by , sr/4)  #0.25 seconds\n",
    "    \n",
    "    wav_roll = np.roll(signal , int(shift_by))\n",
    "    aug_signal = add_white_noise(wav_roll , 0.25)\n",
    "    \n",
    "    file_name = name[:-4] + \"_TimeShifted_\" + str(round(len(signal)/shift_by , 2)) + \".wav\"\n",
    "    file_category = label\n",
    "    \n",
    "    sf.write(save_path + file_name, aug_signal, sr, 'PCM_24')\n",
    "\n",
    "    return file_name,file_category\n",
    "        \n",
    "def augment_signal(signal , sr , name ,label , save_path = \"Audios - Train/\",pitch_shifts=1 ,pitch_shift_by=4,time_shift_factor=4):\n",
    "\n",
    "    file_names = []\n",
    "    file_emotions = []\n",
    "#     aug_signal = add_white_noise(signal,0.25)\n",
    "#     file_names,file_emotions = time_shift_with_gaussian(signal , sr , name , label , save_path)\n",
    "\n",
    "    # PITCH_SHIFT__+__TIME_INVARIANCE\n",
    "    \n",
    "    i = 0\n",
    "    while(i < pitch_shifts):\n",
    "        pitch_shifted_signal_1 = random_gain(pitch_shift(signal , sr , (i+pitch_shift_by)))\n",
    "        pitch_shifted_signal_2 = random_gain(pitch_shift(signal , sr , -(i+pitch_shift_by)))\n",
    "        \n",
    "        file_name_1 = name[:-4] + \"_PitchShiftRG_H\" + str(i) + \".wav\"\n",
    "        file_name_2 = name[:-4] + \"_PitchShiftRG_L\" + str(i) + \".wav\"\n",
    "        \n",
    "        file_names_pss1,file_emotions_pss1 = time_shift_with_gaussian(pitch_shifted_signal_1 , sr , file_name_1 , label , save_path, time_shift_factor)\n",
    "        file_names_pss2,file_emotions_pss2 = time_shift_with_gaussian(pitch_shifted_signal_2 , sr , file_name_2 , label , save_path, time_shift_factor)\n",
    "        \n",
    "#         sf.write(save_path + file_name_1, pitch_shifted_signal_1, sr, 'PCM_24')\n",
    "#         sf.write(save_path + file_name_2, pitch_shifted_signal_2, sr, 'PCM_24')\n",
    "        \n",
    "        file_names.append(save_path + file_names_pss1)\n",
    "        file_names.append(save_path + file_names_pss2)\n",
    "        file_emotions.append(file_emotions_pss1)\n",
    "        file_emotions.append(file_emotions_pss2)\n",
    "        \n",
    "#         file_names.append(save_path + file_name_1)\n",
    "#         file_names.append(save_path + file_name_2)\n",
    "#         file_emotions.append(label)\n",
    "#         file_emotions.append(label)\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    return file_names,file_emotions\n",
    "        \n",
    "    \n",
    "    \n",
    "def augment_signal_time_invariance(signal , sr , name ,label , save_path = \"Audios - Train/\",avg_time_shift_by=5):\n",
    "    new_file_names = []\n",
    "    new_file_categories = []\n",
    "    \n",
    "    i=5\n",
    "    while (i>0):\n",
    "        \n",
    "        file_name,file_category = time_shift_with_gaussian(signal , sr , name , label , save_path, avg_time_shift_by)\n",
    "        \n",
    "        new_file_names.append(save_path + file_name)\n",
    "        new_file_categories.append(label)\n",
    "        \n",
    "        i-=1\n",
    "        \n",
    "    return new_file_names, new_file_categories\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4922/4922 [20:55<00:00,  3.92it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = \"Audios - Train 1/\"\n",
    "new_file_paths = []\n",
    "new_labels = []\n",
    "for i,label in enumerate(tqdm(df_train[\"Emotions\"])):\n",
    "    \n",
    "    path = df_train.iloc[i][\"Path\"]\n",
    "    signal,sr = librosa.load(path)\n",
    "    \n",
    "    name = path[path.rfind(\"/\")+1:]\n",
    "    sf.write(save_path + name, signal, sr, 'PCM_24')\n",
    "    new_file_paths.append(save_path + name)\n",
    "    new_labels.append(label)\n",
    "    \n",
    "    file_names , file_emotions = augment_signal(signal , sr ,\"Aug_\" + name , label , save_path)\n",
    "    \n",
    "    new_file_paths.extend(file_names)\n",
    "    new_labels.extend(file_emotions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame()\n",
    "df_temp[\"Emotions\"] = new_labels\n",
    "df_temp[\"Path\"] = new_file_paths\n",
    "df_temp.to_csv(\"Train-1.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Training-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4922/4922 [18:28<00:00,  4.44it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = \"Audios - Train 2/\"\n",
    "\n",
    "new_file_paths = []\n",
    "new_labels = []\n",
    "for i,label in enumerate(tqdm(df_train[\"Emotions\"])):\n",
    "    \n",
    "    path = df_train.iloc[i][\"Path\"]\n",
    "    signal,sr = librosa.load(path)\n",
    "    \n",
    "    name = path[path.rfind(\"/\")+1:]\n",
    "    sf.write(save_path + name, signal, sr, 'PCM_24')\n",
    "    new_file_paths.append(save_path + name)\n",
    "    new_labels.append(label)\n",
    "    \n",
    "    file_names , file_emotions = augment_signal(signal , sr ,\"Aug_\" + name , label , save_path,pitch_shift_by=2,time_shift_factor=2)\n",
    "    \n",
    "    new_file_paths.extend(file_names)\n",
    "    new_labels.extend(file_emotions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame()\n",
    "df_temp[\"Emotions\"] = new_labels\n",
    "df_temp[\"Path\"] = new_file_paths\n",
    "df_temp.to_csv(\"Train-2.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4922/4922 [05:54<00:00, 13.89it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = \"Audios - Train 3/\"\n",
    "# Emotion Detection Training Data/\n",
    "\n",
    "new_file_paths = []\n",
    "new_labels = []\n",
    "for i,label in enumerate(tqdm(df_train[\"Emotions\"])):\n",
    "    \n",
    "    path = df_train.iloc[i][\"Path\"]\n",
    "    signal,sr = librosa.load(path)\n",
    "    \n",
    "    name = path[path.rfind(\"/\")+1:]\n",
    "    sf.write(save_path + name, signal, sr, 'PCM_24')\n",
    "    new_file_paths.append(save_path + name)\n",
    "    new_labels.append(label)\n",
    "    \n",
    "    file_names , file_emotions = augment_signal_time_invariance(signal , sr ,\"Aug\" + name , label , save_path ,avg_time_shift_by=5)\n",
    "    \n",
    "    new_file_paths.extend(file_names)\n",
    "    new_labels.extend(file_emotions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame()\n",
    "df_temp[\"Emotions\"] = new_labels\n",
    "df_temp[\"Path\"] = new_file_paths\n",
    "df_temp.to_csv(\"Train-3.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_val.to_csv(\"Validation.csv\" , index = False)\n",
    "df_test.to_csv(\"Test.csv\" , index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction Using MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:44:55.850388Z",
     "start_time": "2022-12-23T06:44:55.757960Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_1 = pd.read_csv(\"Train-1.csv\")\n",
    "df_train_2 = pd.read_csv(\"Train-2.csv\")\n",
    "df_train_3 = pd.read_csv(\"Train-3.csv\")\n",
    "df_train = pd.concat([df_train_1 , df_train_2, df_train_3] , axis = 0)\n",
    "df_val = pd.read_csv(\"Validation.csv\")\n",
    "df_test = pd.read_csv(\"Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:44:57.524788Z",
     "start_time": "2022-12-23T06:44:57.506838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTRAIN\n",
      "fear     15180\n",
      "angry    14736\n",
      "happy    14604\n",
      "sad      14544\n",
      "Name: Emotions, dtype: int64\n",
      "\tVALIDATION\n",
      "sad      327\n",
      "happy    321\n",
      "angry    310\n",
      "fear     273\n",
      "Name: Emotions, dtype: int64\n",
      "\tTEST\n",
      "happy    385\n",
      "angry    385\n",
      "fear     385\n",
      "sad      384\n",
      "Name: Emotions, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\tTRAIN\")\n",
    "print(df_train[\"Emotions\"].value_counts())\n",
    "print(\"\\tVALIDATION\")\n",
    "print(df_val[\"Emotions\"].value_counts())\n",
    "print(\"\\tTEST\")\n",
    "print(df_test[\"Emotions\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# _____________________________________\n",
    "# FOR CHECKING MAXIMUM CLIP SIZE\n",
    "# ______________________________________\n",
    "\n",
    "\n",
    "# duration = []\n",
    "# i = 0\n",
    "# while (i < df_train.shape[0]):\n",
    "    \n",
    "#     path = df_train.iloc[i][\"Path\"]\n",
    "#     clip,sr = librosa.load(path)\n",
    "#     duration.append(len(clip)/sr)\n",
    "#     i+=9\n",
    "#     print(i,end=\"\\r\")    \n",
    "\n",
    "# print(\"Max duration at:\" np.argmax(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T06:59:00.092038Z",
     "start_time": "2022-12-23T06:59:00.079908Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_white_noise(signal, noise_percentage_factor):\n",
    "    noise = np.random.normal(0, signal.std()+0.050, signal.size)\n",
    "    augmented_signal = signal + noise * noise_percentage_factor\n",
    "    return augmented_signal\n",
    "\n",
    "\n",
    "def get_mfcc(audio , sr , num_mfcc=15 , n_fft=2048 , hop_length=512 , plot = False):\n",
    "    mfcc = librosa.feature.mfcc(y=audio , sr=sr , n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    del_mfcc = librosa.feature.delta(mfcc)\n",
    "    del2_mfcc = librosa.feature.delta(mfcc, order = 2)\n",
    "    features = np.concatenate([mfcc , del_mfcc, del2_mfcc])\n",
    "    \n",
    "    \n",
    "    if (plot == True):\n",
    "        librosa.display.specshow(mfcc, sr=sr, hop_length=hop_length)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"MFCC coefficients\")\n",
    "        plt.colorbar()\n",
    "        plt.title(\"MFCCs\")\n",
    "\n",
    "        # show plots\n",
    "        plt.show()\n",
    "        \n",
    "    return features.T\n",
    "\n",
    "def get_audio_features(path , mfcc_coeff = 15 , max_duration = 154628 , to_plot = False) :\n",
    "    \n",
    "    clip,sr = librosa.load(path)\n",
    "\n",
    "    if (clip.shape[0] < max_duration):\n",
    "        while(clip.shape[0] < max_duration):\n",
    "            padding = clip[ : max_duration - clip.shape[0]]\n",
    "            clip = np.append(clip , padding)\n",
    "        \n",
    "    elif(clip.shape[0] > max_duration):\n",
    "        clip = clip[:154628]\n",
    "        \n",
    "    data = get_mfcc(np.array(clip),sr, num_mfcc = mfcc_coeff , plot = to_plot) \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Train-1 and Train-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:28:19.959062Z",
     "start_time": "2022-12-23T06:59:04.730165Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59064/59064 [29:13<00:00, 33.69it/s]\n"
     ]
    }
   ],
   "source": [
    "og_data = []\n",
    "for path in tqdm(df_train[\"Path\"]):\n",
    "    data = get_audio_features(path = path)\n",
    "    og_data.append(data)\n",
    "    \n",
    "X_train = np.array(og_data) \n",
    "# np.save(\"Training Data - Extracted Features - MFCC_25\" , X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:31:13.611982Z",
     "start_time": "2022-12-23T07:31:10.878020Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(\"Training Data - Extracted Features - MFCC_15\" , X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Validation & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:33:47.272191Z",
     "start_time": "2022-12-23T07:31:16.977562Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1231/1231 [02:30<00:00,  8.20it/s]\n"
     ]
    }
   ],
   "source": [
    "val_data = []\n",
    "for path in tqdm(df_val[\"Path\"]):\n",
    "    data = get_audio_features(path = path)\n",
    "    val_data.append(data)\n",
    "    \n",
    "X_val = np.array(val_data) \n",
    "np.save(\"Validation Data - Extracted Features - MFCC_15\" , X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:37:02.058965Z",
     "start_time": "2022-12-23T07:33:53.302774Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1539/1539 [03:08<00:00,  8.16it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "for path in tqdm(df_test[\"Path\"]):\n",
    "    data = get_audio_features(path = path)\n",
    "    test_data.append(data)\n",
    "    \n",
    "X_test = np.array(test_data) \n",
    "np.save(\"Test Data - Extracted Features - MFCC_15\" , X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### For Train-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29532/29532 [18:06<00:00, 27.18it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"Train-3.csv\")\n",
    "\n",
    "mfcc_coeff = 25\n",
    "og_data = []\n",
    "for path in tqdm(df_train[\"Path\"]):\n",
    "    data = get_audio_features(path = path , mfcc_coeff=mfcc_coeff)\n",
    "    og_data.append(data)\n",
    "    \n",
    "X_train = np.array(og_data) \n",
    "np.save(\"Training Data3 - Extracted Features - MFCC_25\" , X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LSTM-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:15.302031Z",
     "start_time": "2022-12-23T07:57:03.430Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat([pd.read_csv(\"Train-1.csv\") , pd.read_csv(\"Train-2.csv\") , pd.read_csv(\"Train-3.csv\")] , axis = 0)\n",
    "df_val = pd.read_csv(\"Validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:15.308015Z",
     "start_time": "2022-12-23T07:57:03.730Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.load(\"Training Data - Extracted Features - MFCC_15.npy\")\n",
    "y_train = df_train[\"Emotions\"] \n",
    "\n",
    "X_val = np.load(\"Validation Data - Extracted Features - MFCC_15.npy\")\n",
    "y_val = df_val[\"Emotions\"]\n",
    "\n",
    "one_hot_encoder = preprocessing.LabelBinarizer()\n",
    "one_hot_encoder.fit(df_train[\"Emotions\"].unique())\n",
    "\n",
    "y_train = one_hot_encoder.transform(y_train)\n",
    "y_val = one_hot_encoder.transform(y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:15.312661Z",
     "start_time": "2022-12-23T07:57:04.655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (59064, 303, 45)\n",
      "Val: (1231, 303, 45)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\" , X_train.shape)\n",
    "print(\"Val:\" , X_val.shape)\n",
    "# print(\"Test:\" , X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:15.316650Z",
     "start_time": "2022-12-23T07:57:05.005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes:\" , len(one_hot_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302.0078125"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "154628/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:15.322166Z",
     "start_time": "2022-12-23T07:57:05.380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 45)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:57:15.326154Z",
     "start_time": "2022-12-23T07:57:06.167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# import tensorflow.keras.backend as K \n",
    "# K._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:40:00.157101Z",
     "start_time": "2022-12-23T07:39:58.422850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 303, 50)           19200     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 25)                7600      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                1300      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                1530      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                620       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 20)               80        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,584\n",
      "Trainable params: 30,544\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(LSTM(50,input_shape=input_shape , return_sequences=True))\n",
    "model.add(LSTM(25))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:40:00.867972Z",
     "start_time": "2022-12-23T07:40:00.848941Z"
    }
   },
   "outputs": [],
   "source": [
    "# precision = tf.keras.metrics.Precision()\n",
    "# recall = tf.keras.metrics.Recall()\n",
    "model.compile(optimizer='adam',loss='CategoricalCrossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:40:01.870510Z",
     "start_time": "2022-12-23T07:40:01.860494Z"
    }
   },
   "outputs": [],
   "source": [
    "# gpu_device = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# tf.config.experimental.set_memory_growth(gpu_device[0], True)\n",
    "# # gpu_device[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:52:05.208383Z",
     "start_time": "2022-12-23T07:49:37.334094Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.9431 - accuracy: 0.6377 - val_loss: 0.9078 - val_accuracy: 0.6515\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.9164 - accuracy: 0.6037 - val_loss: 0.8666 - val_accuracy: 0.6288\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.8165 - accuracy: 0.6461 - val_loss: 0.8711 - val_accuracy: 0.6515\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.7951 - accuracy: 0.6607 - val_loss: 0.8695 - val_accuracy: 0.6369\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.7519 - accuracy: 0.6771 - val_loss: 0.8417 - val_accuracy: 0.6539\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.7290 - accuracy: 0.6843 - val_loss: 0.8693 - val_accuracy: 0.6507\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 3s 114ms/step - loss: 0.6971 - accuracy: 0.6949 - val_loss: 0.9164 - val_accuracy: 0.6320\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.6756 - accuracy: 0.7096 - val_loss: 0.8964 - val_accuracy: 0.6450\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.6631 - accuracy: 0.7115 - val_loss: 0.8898 - val_accuracy: 0.6499\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.6463 - accuracy: 0.7215 - val_loss: 0.9396 - val_accuracy: 0.6434\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.6373 - accuracy: 0.7275 - val_loss: 0.9716 - val_accuracy: 0.6442\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.6244 - accuracy: 0.7301 - val_loss: 0.9658 - val_accuracy: 0.6491\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.6303 - accuracy: 0.7244 - val_loss: 0.9994 - val_accuracy: 0.6296\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.6275 - accuracy: 0.7304 - val_loss: 1.0120 - val_accuracy: 0.6231\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.6257 - accuracy: 0.7301 - val_loss: 1.0197 - val_accuracy: 0.6296\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.6324 - accuracy: 0.7256 - val_loss: 1.0544 - val_accuracy: 0.6361\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.6333 - accuracy: 0.7277 - val_loss: 0.9243 - val_accuracy: 0.6572\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.6137 - accuracy: 0.7363 - val_loss: 0.9102 - val_accuracy: 0.6596\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.5887 - accuracy: 0.7452 - val_loss: 0.9332 - val_accuracy: 0.6677\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.5802 - accuracy: 0.7510 - val_loss: 0.9224 - val_accuracy: 0.6596\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.5769 - accuracy: 0.7526 - val_loss: 1.0272 - val_accuracy: 0.6531\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 3s 111ms/step - loss: 0.5809 - accuracy: 0.7515 - val_loss: 1.0124 - val_accuracy: 0.6645\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.5718 - accuracy: 0.7585 - val_loss: 1.0047 - val_accuracy: 0.6621\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.5802 - accuracy: 0.7516 - val_loss: 0.9841 - val_accuracy: 0.6556\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.5605 - accuracy: 0.7601 - val_loss: 0.9471 - val_accuracy: 0.6409\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.5654 - accuracy: 0.7608 - val_loss: 0.9676 - val_accuracy: 0.6637\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.5512 - accuracy: 0.7659 - val_loss: 0.9431 - val_accuracy: 0.6661\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 3s 114ms/step - loss: 0.5414 - accuracy: 0.7707 - val_loss: 0.9755 - val_accuracy: 0.6637\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.5389 - accuracy: 0.7699 - val_loss: 0.9660 - val_accuracy: 0.6507\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 3s 114ms/step - loss: 0.5347 - accuracy: 0.7732 - val_loss: 1.0242 - val_accuracy: 0.6507\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.5353 - accuracy: 0.7702 - val_loss: 1.0213 - val_accuracy: 0.6539\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.5352 - accuracy: 0.7707 - val_loss: 0.9906 - val_accuracy: 0.6434\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.5265 - accuracy: 0.7748 - val_loss: 0.9623 - val_accuracy: 0.6507\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.5634 - accuracy: 0.7591 - val_loss: 1.0313 - val_accuracy: 0.6458\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.5672 - accuracy: 0.7586 - val_loss: 1.0595 - val_accuracy: 0.6523\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.5750 - accuracy: 0.7531 - val_loss: 1.0371 - val_accuracy: 0.6466\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 3s 111ms/step - loss: 0.5648 - accuracy: 0.7589 - val_loss: 1.0928 - val_accuracy: 0.6613\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 3s 111ms/step - loss: 0.5553 - accuracy: 0.7617 - val_loss: 1.0913 - val_accuracy: 0.6523\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 3s 111ms/step - loss: 0.5397 - accuracy: 0.7692 - val_loss: 1.0465 - val_accuracy: 0.6645\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.5191 - accuracy: 0.7756 - val_loss: 1.0471 - val_accuracy: 0.6645\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 3s 111ms/step - loss: 0.5077 - accuracy: 0.7848 - val_loss: 1.0077 - val_accuracy: 0.6734\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.4986 - accuracy: 0.7889 - val_loss: 1.1077 - val_accuracy: 0.6580\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 3s 111ms/step - loss: 0.4901 - accuracy: 0.7899 - val_loss: 1.0558 - val_accuracy: 0.6572\n",
      "Epoch 44/50\n",
      "16/30 [===============>..............] - ETA: 1s - loss: 0.5716 - accuracy: 0.7494"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-19934b30b6ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m25000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m40000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m25000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m40000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train[25000:40000], y_train[25000:40000], epochs=50, batch_size=500, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:54:17.708594Z",
     "start_time": "2022-12-23T07:52:12.128738Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "39/39 [==============================] - 6s 110ms/step - loss: 0.9013 - accuracy: 0.6376 - val_loss: 0.8469 - val_accuracy: 0.6629\n",
      "Epoch 2/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.7665 - accuracy: 0.6687 - val_loss: 0.8037 - val_accuracy: 0.6556\n",
      "Epoch 3/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.6997 - accuracy: 0.6971 - val_loss: 0.7963 - val_accuracy: 0.6621\n",
      "Epoch 4/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.6619 - accuracy: 0.7113 - val_loss: 0.8054 - val_accuracy: 0.6564\n",
      "Epoch 5/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.6459 - accuracy: 0.7171 - val_loss: 0.8393 - val_accuracy: 0.6539\n",
      "Epoch 6/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.6265 - accuracy: 0.7319 - val_loss: 0.8490 - val_accuracy: 0.6466\n",
      "Epoch 7/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.6063 - accuracy: 0.7416 - val_loss: 0.8678 - val_accuracy: 0.6474\n",
      "Epoch 8/50\n",
      "39/39 [==============================] - 4s 111ms/step - loss: 0.5901 - accuracy: 0.7495 - val_loss: 0.8849 - val_accuracy: 0.6434\n",
      "Epoch 9/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.5766 - accuracy: 0.7566 - val_loss: 0.8702 - val_accuracy: 0.6580\n",
      "Epoch 10/50\n",
      "39/39 [==============================] - 4s 111ms/step - loss: 0.5700 - accuracy: 0.7590 - val_loss: 0.8637 - val_accuracy: 0.6564\n",
      "Epoch 11/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.5544 - accuracy: 0.7668 - val_loss: 0.8964 - val_accuracy: 0.6458\n",
      "Epoch 12/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.5440 - accuracy: 0.7727 - val_loss: 0.8878 - val_accuracy: 0.6507\n",
      "Epoch 13/50\n",
      "39/39 [==============================] - 4s 111ms/step - loss: 0.5233 - accuracy: 0.7828 - val_loss: 0.8823 - val_accuracy: 0.6564\n",
      "Epoch 14/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.5199 - accuracy: 0.7823 - val_loss: 0.8768 - val_accuracy: 0.6694\n",
      "Epoch 15/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.5177 - accuracy: 0.7844 - val_loss: 0.8627 - val_accuracy: 0.6686\n",
      "Epoch 16/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.5120 - accuracy: 0.7887 - val_loss: 0.8776 - val_accuracy: 0.6661\n",
      "Epoch 17/50\n",
      "39/39 [==============================] - 4s 111ms/step - loss: 0.5068 - accuracy: 0.7912 - val_loss: 0.9032 - val_accuracy: 0.6637\n",
      "Epoch 18/50\n",
      "39/39 [==============================] - 4s 111ms/step - loss: 0.4987 - accuracy: 0.7948 - val_loss: 0.8963 - val_accuracy: 0.6669\n",
      "Epoch 19/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.4931 - accuracy: 0.7966 - val_loss: 0.9140 - val_accuracy: 0.6661\n",
      "Epoch 20/50\n",
      "39/39 [==============================] - 4s 111ms/step - loss: 0.4825 - accuracy: 0.7999 - val_loss: 0.8888 - val_accuracy: 0.6775\n",
      "Epoch 21/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.4809 - accuracy: 0.7999 - val_loss: 0.9478 - val_accuracy: 0.6629\n",
      "Epoch 22/50\n",
      "39/39 [==============================] - 4s 113ms/step - loss: 0.5140 - accuracy: 0.7849 - val_loss: 0.9130 - val_accuracy: 0.6710\n",
      "Epoch 23/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.5478 - accuracy: 0.7697 - val_loss: 0.9672 - val_accuracy: 0.6450\n",
      "Epoch 24/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.5153 - accuracy: 0.7807 - val_loss: 0.9555 - val_accuracy: 0.6645\n",
      "Epoch 25/50\n",
      "39/39 [==============================] - 4s 111ms/step - loss: 0.4834 - accuracy: 0.7941 - val_loss: 1.0049 - val_accuracy: 0.6515\n",
      "Epoch 26/50\n",
      "39/39 [==============================] - 4s 111ms/step - loss: 0.4879 - accuracy: 0.7972 - val_loss: 0.9469 - val_accuracy: 0.6564\n",
      "Epoch 27/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.4799 - accuracy: 0.7977 - val_loss: 0.9638 - val_accuracy: 0.6621\n",
      "Epoch 28/50\n",
      "39/39 [==============================] - 4s 110ms/step - loss: 0.4798 - accuracy: 0.7980 - val_loss: 1.0497 - val_accuracy: 0.6450\n",
      "Epoch 29/50\n",
      "22/39 [===============>..............] - ETA: 1s - loss: 0.4892 - accuracy: 0.7965"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-90fc547ace9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m40000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m40000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train[40000:], y_train[40000:], epochs=50, batch_size=500, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:54:42.764711Z",
     "start_time": "2022-12-23T07:54:35.320279Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_LSTM_v3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_LSTM_v3\\assets\n"
     ]
    }
   ],
   "source": [
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(physical_devices)\n",
    "model.save(\"Emotion_Detector_LSTM_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training CNN-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:55:04.926689Z",
     "start_time": "2022-12-23T07:55:04.519975Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 303, 128)          28928     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 75, 128)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 75, 128)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 75, 64)            41024     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 37, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 37, 64)            0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2368)              0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2368)             9472      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 25)                59225     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 104       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,753\n",
      "Trainable params: 134,017\n",
      "Non-trainable params: 4,736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Conv1D(128, 5,padding='same',input_shape=input_shape, activation = \"relu\"))\n",
    "model.add(layers.MaxPooling1D(pool_size=(4)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv1D(64, 5,padding='same' , activation = \"relu\"))\n",
    "model.add(layers.MaxPooling1D(pool_size=(2)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(25,activation = \"relu\"))\n",
    "model.add(layers.Dense(4,activation = \"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T07:56:42.161869Z",
     "start_time": "2022-12-23T07:56:31.733389Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50/50 [==============================] - 5s 89ms/step - loss: 1.1889 - accuracy: 0.6392 - val_loss: 1.2501 - val_accuracy: 0.6637\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.8079 - accuracy: 0.6731 - val_loss: 1.0800 - val_accuracy: 0.6791\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.7147 - accuracy: 0.6956 - val_loss: 0.9499 - val_accuracy: 0.7002\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.6562 - accuracy: 0.7236 - val_loss: 0.9539 - val_accuracy: 0.7051\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.6113 - accuracy: 0.7368 - val_loss: 0.9406 - val_accuracy: 0.7051\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.5984 - accuracy: 0.7442 - val_loss: 0.9499 - val_accuracy: 0.7165\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.5653 - accuracy: 0.7612 - val_loss: 0.9277 - val_accuracy: 0.7149\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.5345 - accuracy: 0.7746 - val_loss: 0.9733 - val_accuracy: 0.7206\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.5130 - accuracy: 0.7816 - val_loss: 0.8723 - val_accuracy: 0.7287\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.5000 - accuracy: 0.7882 - val_loss: 0.9269 - val_accuracy: 0.7157\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - 4s 82ms/step - loss: 0.4772 - accuracy: 0.7993 - val_loss: 1.0086 - val_accuracy: 0.6970\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.4594 - accuracy: 0.8059 - val_loss: 0.9530 - val_accuracy: 0.7124\n",
      "Epoch 13/20\n",
      "50/50 [==============================] - 4s 82ms/step - loss: 0.4445 - accuracy: 0.8112 - val_loss: 1.4752 - val_accuracy: 0.7132\n",
      "Epoch 14/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.4453 - accuracy: 0.8127 - val_loss: 0.9750 - val_accuracy: 0.7335\n",
      "Epoch 15/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.4282 - accuracy: 0.8220 - val_loss: 1.0300 - val_accuracy: 0.7254\n",
      "Epoch 16/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.4099 - accuracy: 0.8276 - val_loss: 1.0904 - val_accuracy: 0.7011\n",
      "Epoch 17/20\n",
      "50/50 [==============================] - 4s 82ms/step - loss: 0.3959 - accuracy: 0.8335 - val_loss: 1.0259 - val_accuracy: 0.7116\n",
      "Epoch 18/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.4007 - accuracy: 0.8329 - val_loss: 1.0664 - val_accuracy: 0.7271\n",
      "Epoch 19/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.3844 - accuracy: 0.8414 - val_loss: 1.0826 - val_accuracy: 0.7157\n",
      "Epoch 20/20\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.3623 - accuracy: 0.8500 - val_loss: 1.1300 - val_accuracy: 0.7092\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='CategoricalCrossentropy',metrics=['accuracy'])\n",
    "history = model.fit(X_train[:25000], y_train[:25000], epochs=20, batch_size=500, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30/30 [==============================] - 3s 86ms/step - loss: 0.8587 - accuracy: 0.7505 - val_loss: 2.1574 - val_accuracy: 0.6954\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.6449 - accuracy: 0.7457 - val_loss: 1.9185 - val_accuracy: 0.6905\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.5276 - accuracy: 0.7893 - val_loss: 1.5135 - val_accuracy: 0.7035\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.4605 - accuracy: 0.8113 - val_loss: 1.6803 - val_accuracy: 0.7067\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.4109 - accuracy: 0.8379 - val_loss: 1.7433 - val_accuracy: 0.7051\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.3641 - accuracy: 0.8521 - val_loss: 1.4806 - val_accuracy: 0.7084\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.3359 - accuracy: 0.8654 - val_loss: 1.4028 - val_accuracy: 0.7067\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.3032 - accuracy: 0.8787 - val_loss: 1.4364 - val_accuracy: 0.7132\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.2806 - accuracy: 0.8877 - val_loss: 1.4858 - val_accuracy: 0.7157\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.2600 - accuracy: 0.8951 - val_loss: 1.5308 - val_accuracy: 0.7076\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.2479 - accuracy: 0.9020 - val_loss: 1.9866 - val_accuracy: 0.6946\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.2402 - accuracy: 0.9045 - val_loss: 1.5557 - val_accuracy: 0.7051\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 3s 85ms/step - loss: 0.2288 - accuracy: 0.9085 - val_loss: 1.6291 - val_accuracy: 0.6929\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.2183 - accuracy: 0.9135 - val_loss: 1.5691 - val_accuracy: 0.6970\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.2024 - accuracy: 0.9190 - val_loss: 1.6961 - val_accuracy: 0.6937\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1941 - accuracy: 0.9254 - val_loss: 1.6378 - val_accuracy: 0.6937\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1879 - accuracy: 0.9273 - val_loss: 1.7410 - val_accuracy: 0.6767\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1821 - accuracy: 0.9277 - val_loss: 1.5716 - val_accuracy: 0.7076\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1622 - accuracy: 0.9363 - val_loss: 1.6211 - val_accuracy: 0.6978\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 3s 85ms/step - loss: 0.1614 - accuracy: 0.9373 - val_loss: 1.7370 - val_accuracy: 0.6824\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1586 - accuracy: 0.9383 - val_loss: 1.4888 - val_accuracy: 0.7149\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 3s 85ms/step - loss: 0.1493 - accuracy: 0.9403 - val_loss: 1.4741 - val_accuracy: 0.7271\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1468 - accuracy: 0.9434 - val_loss: 1.6376 - val_accuracy: 0.7173\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1354 - accuracy: 0.9484 - val_loss: 1.6138 - val_accuracy: 0.7238\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1409 - accuracy: 0.9451 - val_loss: 1.6768 - val_accuracy: 0.7027\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1295 - accuracy: 0.9491 - val_loss: 1.6224 - val_accuracy: 0.7279\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1279 - accuracy: 0.9503 - val_loss: 1.5992 - val_accuracy: 0.7165\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.1238 - accuracy: 0.9535 - val_loss: 1.7173 - val_accuracy: 0.7043\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.1164 - accuracy: 0.9553 - val_loss: 1.7341 - val_accuracy: 0.7132\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 3s 83ms/step - loss: 0.1151 - accuracy: 0.9531 - val_loss: 1.6346 - val_accuracy: 0.7206\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.1108 - accuracy: 0.9601 - val_loss: 1.7353 - val_accuracy: 0.6978\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.1042 - accuracy: 0.9605 - val_loss: 1.6827 - val_accuracy: 0.7141\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0993 - accuracy: 0.9626 - val_loss: 1.7809 - val_accuracy: 0.7206\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.1003 - accuracy: 0.9620 - val_loss: 1.8941 - val_accuracy: 0.7100\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 3s 83ms/step - loss: 0.1002 - accuracy: 0.9609 - val_loss: 1.7354 - val_accuracy: 0.7157\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0986 - accuracy: 0.9633 - val_loss: 1.8778 - val_accuracy: 0.7027\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0909 - accuracy: 0.9667 - val_loss: 1.7409 - val_accuracy: 0.7173\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0923 - accuracy: 0.9633 - val_loss: 1.8208 - val_accuracy: 0.7092\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0883 - accuracy: 0.9675 - val_loss: 1.7868 - val_accuracy: 0.7157\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.0887 - accuracy: 0.9670 - val_loss: 1.7615 - val_accuracy: 0.7165\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0864 - accuracy: 0.9679 - val_loss: 1.8060 - val_accuracy: 0.7238\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0878 - accuracy: 0.9675 - val_loss: 1.9459 - val_accuracy: 0.7116\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0924 - accuracy: 0.9654 - val_loss: 1.8452 - val_accuracy: 0.7076\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0895 - accuracy: 0.9677 - val_loss: 1.8858 - val_accuracy: 0.7076\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 3s 83ms/step - loss: 0.0871 - accuracy: 0.9677 - val_loss: 1.8547 - val_accuracy: 0.7189\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0865 - accuracy: 0.9679 - val_loss: 1.8375 - val_accuracy: 0.7141\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 2s 83ms/step - loss: 0.0852 - accuracy: 0.9684 - val_loss: 1.7969 - val_accuracy: 0.7157\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 2s 84ms/step - loss: 0.0846 - accuracy: 0.9695 - val_loss: 1.8068 - val_accuracy: 0.7352\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 3s 84ms/step - loss: 0.0764 - accuracy: 0.9712 - val_loss: 1.7578 - val_accuracy: 0.7214\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 3s 86ms/step - loss: 0.0699 - accuracy: 0.9757 - val_loss: 1.7248 - val_accuracy: 0.7197\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train[25000:40000], y_train[25000:40000], epochs=50, batch_size=500, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 1.0784 - accuracy: 0.7336 - val_loss: 2.0320 - val_accuracy: 0.6409\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.6159 - accuracy: 0.7681 - val_loss: 2.0339 - val_accuracy: 0.6807\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.4659 - accuracy: 0.8132 - val_loss: 1.5479 - val_accuracy: 0.7067\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.3655 - accuracy: 0.8525 - val_loss: 1.4302 - val_accuracy: 0.7181\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.3072 - accuracy: 0.8756 - val_loss: 1.5020 - val_accuracy: 0.7076\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.2814 - accuracy: 0.8867 - val_loss: 1.4955 - val_accuracy: 0.7132\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.2377 - accuracy: 0.9081 - val_loss: 1.5335 - val_accuracy: 0.7197\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1999 - accuracy: 0.9203 - val_loss: 1.4661 - val_accuracy: 0.7197\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.1894 - accuracy: 0.9224 - val_loss: 1.4922 - val_accuracy: 0.7189\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1791 - accuracy: 0.9297 - val_loss: 1.5681 - val_accuracy: 0.7214\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1567 - accuracy: 0.9412 - val_loss: 1.5554 - val_accuracy: 0.7222\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1403 - accuracy: 0.9474 - val_loss: 1.5225 - val_accuracy: 0.7319\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1331 - accuracy: 0.9501 - val_loss: 1.5395 - val_accuracy: 0.7173\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.1191 - accuracy: 0.9570 - val_loss: 1.6083 - val_accuracy: 0.7197\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1151 - accuracy: 0.9577 - val_loss: 1.5858 - val_accuracy: 0.7157\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.1019 - accuracy: 0.9621 - val_loss: 1.6654 - val_accuracy: 0.7157\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0942 - accuracy: 0.9661 - val_loss: 1.6702 - val_accuracy: 0.7189\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0871 - accuracy: 0.9679 - val_loss: 1.7855 - val_accuracy: 0.7197\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0888 - accuracy: 0.9674 - val_loss: 1.6527 - val_accuracy: 0.7181\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0788 - accuracy: 0.9709 - val_loss: 1.6360 - val_accuracy: 0.7271\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0715 - accuracy: 0.9744 - val_loss: 1.6298 - val_accuracy: 0.7149\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0672 - accuracy: 0.9750 - val_loss: 1.7443 - val_accuracy: 0.7116\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 0.0680 - accuracy: 0.9759 - val_loss: 1.7128 - val_accuracy: 0.7295\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.0673 - accuracy: 0.9756 - val_loss: 1.7965 - val_accuracy: 0.7165\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.0613 - accuracy: 0.9793 - val_loss: 1.8117 - val_accuracy: 0.7181\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0538 - accuracy: 0.9815 - val_loss: 1.7466 - val_accuracy: 0.7271\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0512 - accuracy: 0.9834 - val_loss: 1.7429 - val_accuracy: 0.7262\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0476 - accuracy: 0.9833 - val_loss: 1.7563 - val_accuracy: 0.7230\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0509 - accuracy: 0.9822 - val_loss: 1.7158 - val_accuracy: 0.7084\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0467 - accuracy: 0.9836 - val_loss: 1.7793 - val_accuracy: 0.7051\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0462 - accuracy: 0.9842 - val_loss: 1.8769 - val_accuracy: 0.7165\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0419 - accuracy: 0.9863 - val_loss: 1.7766 - val_accuracy: 0.7230\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0421 - accuracy: 0.9854 - val_loss: 1.8544 - val_accuracy: 0.7165\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0394 - accuracy: 0.9858 - val_loss: 1.8926 - val_accuracy: 0.7141\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0382 - accuracy: 0.9885 - val_loss: 1.9477 - val_accuracy: 0.7246\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0407 - accuracy: 0.9862 - val_loss: 2.0595 - val_accuracy: 0.6913\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0376 - accuracy: 0.9873 - val_loss: 1.8869 - val_accuracy: 0.7100\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.0324 - accuracy: 0.9891 - val_loss: 2.0132 - val_accuracy: 0.7222\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0316 - accuracy: 0.9903 - val_loss: 1.9516 - val_accuracy: 0.7254\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0290 - accuracy: 0.9905 - val_loss: 2.1979 - val_accuracy: 0.6864\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0316 - accuracy: 0.9911 - val_loss: 2.0947 - val_accuracy: 0.7084\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0359 - accuracy: 0.9885 - val_loss: 1.9569 - val_accuracy: 0.7108\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0305 - accuracy: 0.9901 - val_loss: 1.9039 - val_accuracy: 0.7059\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0289 - accuracy: 0.9906 - val_loss: 1.9865 - val_accuracy: 0.7051\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 0.0276 - accuracy: 0.9907 - val_loss: 1.8645 - val_accuracy: 0.7165\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0255 - accuracy: 0.9920 - val_loss: 1.9246 - val_accuracy: 0.7206\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0254 - accuracy: 0.9922 - val_loss: 1.9621 - val_accuracy: 0.7230\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0255 - accuracy: 0.9923 - val_loss: 1.9370 - val_accuracy: 0.7165\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 1.9236 - val_accuracy: 0.7238\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 1.9508 - val_accuracy: 0.7254\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train[40000:50000], y_train[40000:50000], epochs=50, batch_size=500, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "91/91 [==============================] - 3s 25ms/step - loss: 1.2314 - accuracy: 0.5441 - val_loss: 1.8593 - val_accuracy: 0.4679\n",
      "Epoch 2/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.8699 - accuracy: 0.6134 - val_loss: 1.0803 - val_accuracy: 0.5727\n",
      "Epoch 3/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.7555 - accuracy: 0.6677 - val_loss: 1.0424 - val_accuracy: 0.6296\n",
      "Epoch 4/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.6759 - accuracy: 0.7020 - val_loss: 0.9304 - val_accuracy: 0.6214\n",
      "Epoch 5/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.6234 - accuracy: 0.7287 - val_loss: 1.1106 - val_accuracy: 0.6288\n",
      "Epoch 6/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.5711 - accuracy: 0.7549 - val_loss: 1.0507 - val_accuracy: 0.6523\n",
      "Epoch 7/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.5494 - accuracy: 0.7656 - val_loss: 1.2401 - val_accuracy: 0.6328\n",
      "Epoch 8/50\n",
      "91/91 [==============================] - 2s 25ms/step - loss: 0.5118 - accuracy: 0.7839 - val_loss: 1.0551 - val_accuracy: 0.6515\n",
      "Epoch 9/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.4769 - accuracy: 0.7919 - val_loss: 1.0720 - val_accuracy: 0.6645\n",
      "Epoch 10/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.4385 - accuracy: 0.8144 - val_loss: 1.0611 - val_accuracy: 0.6783\n",
      "Epoch 11/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.4391 - accuracy: 0.8141 - val_loss: 1.0619 - val_accuracy: 0.6661\n",
      "Epoch 12/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.3975 - accuracy: 0.8382 - val_loss: 1.1705 - val_accuracy: 0.6515\n",
      "Epoch 13/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.3623 - accuracy: 0.8490 - val_loss: 1.1369 - val_accuracy: 0.6686\n",
      "Epoch 14/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.3408 - accuracy: 0.8551 - val_loss: 1.2933 - val_accuracy: 0.6807\n",
      "Epoch 15/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.3273 - accuracy: 0.8656 - val_loss: 1.3823 - val_accuracy: 0.6596\n",
      "Epoch 16/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.3068 - accuracy: 0.8768 - val_loss: 1.3039 - val_accuracy: 0.6645\n",
      "Epoch 17/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2873 - accuracy: 0.8837 - val_loss: 1.3224 - val_accuracy: 0.6710\n",
      "Epoch 18/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2467 - accuracy: 0.9005 - val_loss: 1.5593 - val_accuracy: 0.6621\n",
      "Epoch 19/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2968 - accuracy: 0.8799 - val_loss: 1.2709 - val_accuracy: 0.6539\n",
      "Epoch 20/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2909 - accuracy: 0.8822 - val_loss: 1.3940 - val_accuracy: 0.6718\n",
      "Epoch 21/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2598 - accuracy: 0.8961 - val_loss: 1.5946 - val_accuracy: 0.6759\n",
      "Epoch 22/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2272 - accuracy: 0.9093 - val_loss: 1.5916 - val_accuracy: 0.6580\n",
      "Epoch 23/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.2195 - accuracy: 0.9121 - val_loss: 1.6072 - val_accuracy: 0.6539\n",
      "Epoch 24/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.2061 - accuracy: 0.9155 - val_loss: 1.4869 - val_accuracy: 0.6556\n",
      "Epoch 25/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1816 - accuracy: 0.9302 - val_loss: 1.6620 - val_accuracy: 0.6288\n",
      "Epoch 26/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1822 - accuracy: 0.9301 - val_loss: 2.6836 - val_accuracy: 0.6223\n",
      "Epoch 27/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1756 - accuracy: 0.9314 - val_loss: 1.4288 - val_accuracy: 0.6653\n",
      "Epoch 28/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1796 - accuracy: 0.9298 - val_loss: 1.4627 - val_accuracy: 0.6832\n",
      "Epoch 29/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1796 - accuracy: 0.9283 - val_loss: 1.5028 - val_accuracy: 0.6385\n",
      "Epoch 30/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1693 - accuracy: 0.9369 - val_loss: 1.5386 - val_accuracy: 0.6604\n",
      "Epoch 31/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1683 - accuracy: 0.9337 - val_loss: 1.6212 - val_accuracy: 0.6653\n",
      "Epoch 32/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1558 - accuracy: 0.9394 - val_loss: 1.6964 - val_accuracy: 0.6507\n",
      "Epoch 33/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1317 - accuracy: 0.9494 - val_loss: 1.4553 - val_accuracy: 0.7100\n",
      "Epoch 34/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1191 - accuracy: 0.9551 - val_loss: 1.6110 - val_accuracy: 0.6604\n",
      "Epoch 35/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1150 - accuracy: 0.9579 - val_loss: 1.5848 - val_accuracy: 0.6702\n",
      "Epoch 36/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1344 - accuracy: 0.9492 - val_loss: 1.7528 - val_accuracy: 0.6166\n",
      "Epoch 37/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1380 - accuracy: 0.9458 - val_loss: 2.0539 - val_accuracy: 0.6710\n",
      "Epoch 38/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1121 - accuracy: 0.9587 - val_loss: 1.6485 - val_accuracy: 0.6799\n",
      "Epoch 39/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1064 - accuracy: 0.9603 - val_loss: 1.7074 - val_accuracy: 0.6726\n",
      "Epoch 40/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1028 - accuracy: 0.9639 - val_loss: 1.7471 - val_accuracy: 0.6669\n",
      "Epoch 41/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.0887 - accuracy: 0.9662 - val_loss: 1.6759 - val_accuracy: 0.6872\n",
      "Epoch 42/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.0908 - accuracy: 0.9655 - val_loss: 1.9620 - val_accuracy: 0.6491\n",
      "Epoch 43/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.0805 - accuracy: 0.9692 - val_loss: 1.7884 - val_accuracy: 0.6677\n",
      "Epoch 44/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1112 - accuracy: 0.9583 - val_loss: 2.1661 - val_accuracy: 0.6751\n",
      "Epoch 45/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.1053 - accuracy: 0.9600 - val_loss: 1.8954 - val_accuracy: 0.6872\n",
      "Epoch 46/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.0989 - accuracy: 0.9659 - val_loss: 2.0745 - val_accuracy: 0.6669\n",
      "Epoch 47/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1045 - accuracy: 0.9607 - val_loss: 1.9748 - val_accuracy: 0.6710\n",
      "Epoch 48/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.1325 - accuracy: 0.9524 - val_loss: 2.0505 - val_accuracy: 0.6361\n",
      "Epoch 49/50\n",
      "91/91 [==============================] - 2s 24ms/step - loss: 0.0981 - accuracy: 0.9632 - val_loss: 1.9990 - val_accuracy: 0.6653\n",
      "Epoch 50/50\n",
      "91/91 [==============================] - 2s 23ms/step - loss: 0.0859 - accuracy: 0.9689 - val_loss: 2.0199 - val_accuracy: 0.6580\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train[50000:], y_train[50000:], epochs=50, batch_size=100, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_CNN_v3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_CNN_v3\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Emotion_Detector_CNN_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Train-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### LSTM-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"Emotion_Detector_new_v2\")\n",
    "df_train = pd.read_csv(\"Train-3.csv\")\n",
    "df_val = pd.read_csv(\"Validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load(\"Training Data3 - Extracted Features - MFCC_25.npy\")\n",
    "y_train = df_train[\"Emotions\"] \n",
    "\n",
    "X_val = np.load(\"Validation Data - Extracted Features - MFCC_25.npy\")\n",
    "y_val = df_val[\"Emotions\"]\n",
    "\n",
    "one_hot_encoder = preprocessing.LabelBinarizer()\n",
    "one_hot_encoder.fit(df_train[\"Emotions\"].unique())\n",
    "\n",
    "y_train = one_hot_encoder.transform(y_train)\n",
    "y_val = one_hot_encoder.transform(y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (29532, 303, 25)\n",
      "Val: (1231, 303, 25)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\" , X_train.shape)\n",
    "print(\"Val:\" , X_val.shape)\n",
    "# print(\"Test:\" , X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes:\" , len(one_hot_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 25)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 6s 102ms/step - loss: 0.9293 - accuracy: 0.6127 - val_loss: 0.8899 - val_accuracy: 0.6190\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.8208 - accuracy: 0.6490 - val_loss: 0.8614 - val_accuracy: 0.6288\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.7836 - accuracy: 0.6648 - val_loss: 0.8609 - val_accuracy: 0.6255\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.7643 - accuracy: 0.6693 - val_loss: 0.8587 - val_accuracy: 0.6353\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.7480 - accuracy: 0.6788 - val_loss: 0.8384 - val_accuracy: 0.6442\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 6s 102ms/step - loss: 0.7302 - accuracy: 0.6867 - val_loss: 0.8415 - val_accuracy: 0.6418\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.7236 - accuracy: 0.6879 - val_loss: 0.8517 - val_accuracy: 0.6393\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.7105 - accuracy: 0.6939 - val_loss: 0.8373 - val_accuracy: 0.6450\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.7002 - accuracy: 0.6977 - val_loss: 0.8338 - val_accuracy: 0.6426\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6909 - accuracy: 0.7011 - val_loss: 0.8496 - val_accuracy: 0.6393\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6842 - accuracy: 0.7013 - val_loss: 0.8522 - val_accuracy: 0.6450\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 6s 102ms/step - loss: 0.6770 - accuracy: 0.7061 - val_loss: 0.8752 - val_accuracy: 0.6531\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 6s 103ms/step - loss: 0.6691 - accuracy: 0.7086 - val_loss: 0.8769 - val_accuracy: 0.6296\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 6s 102ms/step - loss: 0.6591 - accuracy: 0.7127 - val_loss: 0.8678 - val_accuracy: 0.6385\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6542 - accuracy: 0.7173 - val_loss: 0.8659 - val_accuracy: 0.6344\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 6s 103ms/step - loss: 0.6471 - accuracy: 0.7194 - val_loss: 0.8985 - val_accuracy: 0.6328\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6416 - accuracy: 0.7242 - val_loss: 0.8620 - val_accuracy: 0.6409\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.6361 - accuracy: 0.7275 - val_loss: 0.8601 - val_accuracy: 0.6426\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.6255 - accuracy: 0.7308 - val_loss: 0.8823 - val_accuracy: 0.6361\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6214 - accuracy: 0.7321 - val_loss: 0.8892 - val_accuracy: 0.6328\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6159 - accuracy: 0.7374 - val_loss: 0.8957 - val_accuracy: 0.6328\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6124 - accuracy: 0.7384 - val_loss: 0.9128 - val_accuracy: 0.6361\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.6093 - accuracy: 0.7414 - val_loss: 0.8755 - val_accuracy: 0.6418\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.6011 - accuracy: 0.7441 - val_loss: 0.9140 - val_accuracy: 0.6312\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.6022 - accuracy: 0.7444 - val_loss: 0.8837 - val_accuracy: 0.6409\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.5980 - accuracy: 0.7462 - val_loss: 0.9117 - val_accuracy: 0.6320\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5957 - accuracy: 0.7487 - val_loss: 0.8862 - val_accuracy: 0.6344\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5882 - accuracy: 0.7546 - val_loss: 0.8938 - val_accuracy: 0.6401\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 6s 99ms/step - loss: 0.5858 - accuracy: 0.7562 - val_loss: 0.8814 - val_accuracy: 0.6369\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5761 - accuracy: 0.7589 - val_loss: 0.9094 - val_accuracy: 0.6288\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.5887 - accuracy: 0.7558 - val_loss: 0.9060 - val_accuracy: 0.6434\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 6s 99ms/step - loss: 0.6047 - accuracy: 0.7479 - val_loss: 0.8841 - val_accuracy: 0.6515\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.5810 - accuracy: 0.7587 - val_loss: 0.8954 - val_accuracy: 0.6409\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 6s 99ms/step - loss: 0.5711 - accuracy: 0.7630 - val_loss: 0.8888 - val_accuracy: 0.6409\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 6s 99ms/step - loss: 0.5721 - accuracy: 0.7619 - val_loss: 0.8857 - val_accuracy: 0.6483\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5740 - accuracy: 0.7610 - val_loss: 0.8938 - val_accuracy: 0.6491\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 6s 99ms/step - loss: 0.5679 - accuracy: 0.7636 - val_loss: 0.8909 - val_accuracy: 0.6442\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5625 - accuracy: 0.7662 - val_loss: 0.8984 - val_accuracy: 0.6572\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 6s 99ms/step - loss: 0.5578 - accuracy: 0.7692 - val_loss: 0.9208 - val_accuracy: 0.6531\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5579 - accuracy: 0.7683 - val_loss: 0.8935 - val_accuracy: 0.6564\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5736 - accuracy: 0.7632 - val_loss: 0.8819 - val_accuracy: 0.6653\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5632 - accuracy: 0.7650 - val_loss: 0.9125 - val_accuracy: 0.6483\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5569 - accuracy: 0.7677 - val_loss: 0.9203 - val_accuracy: 0.6596\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5567 - accuracy: 0.7669 - val_loss: 0.9167 - val_accuracy: 0.6450\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5493 - accuracy: 0.7708 - val_loss: 0.9073 - val_accuracy: 0.6531\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.5459 - accuracy: 0.7721 - val_loss: 0.8928 - val_accuracy: 0.6466\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5519 - accuracy: 0.7696 - val_loss: 0.9435 - val_accuracy: 0.6483\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5506 - accuracy: 0.7696 - val_loss: 0.9777 - val_accuracy: 0.6393\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 6s 100ms/step - loss: 0.5504 - accuracy: 0.7699 - val_loss: 0.9792 - val_accuracy: 0.6491\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 6s 101ms/step - loss: 0.5436 - accuracy: 0.7701 - val_loss: 0.9027 - val_accuracy: 0.6548\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=500, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_new_v2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_new_v2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Emotion_Detector_new_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"Emotion_Detector_new_CNN_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60/60 [==============================] - 5s 74ms/step - loss: 0.1719 - accuracy: 0.9345 - val_loss: 1.8509 - val_accuracy: 0.7400\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1574 - accuracy: 0.9402 - val_loss: 1.8037 - val_accuracy: 0.7425\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1505 - accuracy: 0.9427 - val_loss: 1.7518 - val_accuracy: 0.7238\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1606 - accuracy: 0.9394 - val_loss: 2.1389 - val_accuracy: 0.7409\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1509 - accuracy: 0.9436 - val_loss: 2.4488 - val_accuracy: 0.7433\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1516 - accuracy: 0.9429 - val_loss: 2.1439 - val_accuracy: 0.7262\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1472 - accuracy: 0.9451 - val_loss: 2.8443 - val_accuracy: 0.7376\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1707 - accuracy: 0.9361 - val_loss: 3.4290 - val_accuracy: 0.7181\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1447 - accuracy: 0.9462 - val_loss: 3.0728 - val_accuracy: 0.7238\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1443 - accuracy: 0.9449 - val_loss: 4.2604 - val_accuracy: 0.7189\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1346 - accuracy: 0.9497 - val_loss: 3.7890 - val_accuracy: 0.6937\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1355 - accuracy: 0.9490 - val_loss: 1.7121 - val_accuracy: 0.7400\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1310 - accuracy: 0.9513 - val_loss: 1.8253 - val_accuracy: 0.7319\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1342 - accuracy: 0.9484 - val_loss: 1.9104 - val_accuracy: 0.7173\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 4s 69ms/step - loss: 0.1368 - accuracy: 0.9482 - val_loss: 1.8051 - val_accuracy: 0.7189\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 0.1285 - accuracy: 0.9538 - val_loss: 1.9958 - val_accuracy: 0.7287\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1292 - accuracy: 0.9515 - val_loss: 2.2557 - val_accuracy: 0.7417\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1256 - accuracy: 0.9529 - val_loss: 2.3019 - val_accuracy: 0.7092\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1266 - accuracy: 0.9527 - val_loss: 2.3012 - val_accuracy: 0.7230\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1369 - accuracy: 0.9485 - val_loss: 2.0892 - val_accuracy: 0.7246\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1290 - accuracy: 0.9530 - val_loss: 1.9340 - val_accuracy: 0.7189\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1538 - accuracy: 0.9417 - val_loss: 2.1136 - val_accuracy: 0.7262\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1390 - accuracy: 0.9470 - val_loss: 2.3978 - val_accuracy: 0.7100\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1448 - accuracy: 0.9458 - val_loss: 2.3473 - val_accuracy: 0.7311\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1270 - accuracy: 0.9523 - val_loss: 1.9089 - val_accuracy: 0.7344\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1175 - accuracy: 0.9559 - val_loss: 2.0087 - val_accuracy: 0.7295\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1155 - accuracy: 0.9572 - val_loss: 2.3009 - val_accuracy: 0.7271\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1335 - accuracy: 0.9491 - val_loss: 1.9195 - val_accuracy: 0.7189\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1239 - accuracy: 0.9547 - val_loss: 1.6412 - val_accuracy: 0.7425\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1181 - accuracy: 0.9565 - val_loss: 1.7866 - val_accuracy: 0.7197\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1140 - accuracy: 0.9580 - val_loss: 1.8869 - val_accuracy: 0.7197\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1136 - accuracy: 0.9590 - val_loss: 1.9851 - val_accuracy: 0.7181\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1063 - accuracy: 0.9605 - val_loss: 2.0592 - val_accuracy: 0.7189\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1077 - accuracy: 0.9596 - val_loss: 1.9267 - val_accuracy: 0.7392\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1042 - accuracy: 0.9617 - val_loss: 1.9741 - val_accuracy: 0.7214\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1128 - accuracy: 0.9591 - val_loss: 1.9374 - val_accuracy: 0.7335\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1070 - accuracy: 0.9607 - val_loss: 2.2587 - val_accuracy: 0.7303\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1053 - accuracy: 0.9613 - val_loss: 2.1567 - val_accuracy: 0.7189\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1030 - accuracy: 0.9618 - val_loss: 2.0418 - val_accuracy: 0.7352\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1048 - accuracy: 0.9618 - val_loss: 2.0057 - val_accuracy: 0.7287\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1092 - accuracy: 0.9599 - val_loss: 2.1925 - val_accuracy: 0.7173\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1156 - accuracy: 0.9572 - val_loss: 2.0124 - val_accuracy: 0.7295\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1113 - accuracy: 0.9592 - val_loss: 2.2508 - val_accuracy: 0.7271\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1389 - accuracy: 0.9495 - val_loss: 2.3905 - val_accuracy: 0.7335\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1045 - accuracy: 0.9614 - val_loss: 2.6854 - val_accuracy: 0.7271\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1222 - accuracy: 0.9556 - val_loss: 2.4408 - val_accuracy: 0.7287\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1166 - accuracy: 0.9577 - val_loss: 2.4415 - val_accuracy: 0.7254\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1060 - accuracy: 0.9600 - val_loss: 1.8488 - val_accuracy: 0.7254\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1017 - accuracy: 0.9634 - val_loss: 1.6324 - val_accuracy: 0.7238\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 4s 68ms/step - loss: 0.1676 - accuracy: 0.9412 - val_loss: 1.6875 - val_accuracy: 0.7360\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=500, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_new_CNN_v2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Emotion_Detector_new_CNN_v2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Emotion_Detector_new_CNN_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### LSTM-CNN using Train-1 & Train-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### LSTM-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_train_1 = pd.read_csv(\"Train-1.csv\")\n",
    "# df_train_2 = pd.read_csv(\"Train-2.csv\")\n",
    "df_train = pd.concat([pd.read_csv(\"Train-1.csv\"),\n",
    "                      pd.read_csv(\"Train-2.csv\"),] , axis = 0)\n",
    "df_val = pd.read_csv(\"Validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load(\"Training Data3 - Extracted Features - MFCC_25.npy\")\n",
    "y_train = df_train[\"Emotions\"] \n",
    "\n",
    "X_val = np.load(\"Validation Data - Extracted Features - MFCC_25.npy\")\n",
    "y_val = df_val[\"Emotions\"]\n",
    "\n",
    "one_hot_encoder = preprocessing.LabelBinarizer()\n",
    "one_hot_encoder.fit(df_train[\"Emotions\"].unique())\n",
    "\n",
    "y_train = one_hot_encoder.transform(y_train)\n",
    "y_val = one_hot_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (29532, 303, 25)\n",
      "Val: (1231, 303, 25)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\" , X_train.shape)\n",
    "print(\"Val:\" , X_val.shape)\n",
    "# print(\"Test:\" , X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes:\" , len(one_hot_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 303, 25)           5100      \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 303, 64)           8064      \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 303, 32)           10272     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 303, 32)           0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 25)                5800      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 20)                520       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 20)               80        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                210       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 4)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,090\n",
      "Trainable params: 30,050\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(25 , return_sequences = True , input_shape = input_shape))\n",
    "model.add(layers.Conv1D(64, 5, padding = \"same\" , activation = \"relu\"))\n",
    "model.add(layers.Conv1D(32, 5, padding = \"same\", activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "# model.add(layers.Flatten())\n",
    "model.add(layers.LSTM(25))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# model.build(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30/30 [==============================] - 10s 227ms/step - loss: 1.3903 - accuracy: 0.2490 - val_loss: 1.3857 - val_accuracy: 0.2616\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 6s 196ms/step - loss: 1.3865 - accuracy: 0.2547 - val_loss: 1.3865 - val_accuracy: 0.2218\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 6s 194ms/step - loss: 1.3863 - accuracy: 0.2534 - val_loss: 1.3867 - val_accuracy: 0.2226\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 6s 196ms/step - loss: 1.3861 - accuracy: 0.2573 - val_loss: 1.3870 - val_accuracy: 0.2218\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 6s 195ms/step - loss: 1.3861 - accuracy: 0.2565 - val_loss: 1.3870 - val_accuracy: 0.2218\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6dac35f54ec9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'CategoricalCrossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='CategoricalCrossentropy',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=1000, validation_data=(X_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save(\"Emotion_Detector_LSTM_CNN_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"Emotion_Detector_LSTM_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"Test.csv\")\n",
    "X_test = np.load(\"Test Data - Extracted Features - MFCC_15.npy\")\n",
    "y_test = df_test[\"Emotions\"]\n",
    "\n",
    "one_hot_encoder = preprocessing.LabelBinarizer()\n",
    "one_hot_encoder.fit(pd.read_csv(\"Train-1.csv\")[\"Emotions\"].unique())\n",
    "y_test = one_hot_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['angry', 'fear', 'happy', 'sad']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(one_hot_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 2s 18ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.77      0.75      0.76       385\n",
      "        fear       0.72      0.33      0.45       385\n",
      "       happy       0.54      0.70      0.61       385\n",
      "         sad       0.66      0.85      0.74       384\n",
      "\n",
      "    accuracy                           0.66      1539\n",
      "   macro avg       0.67      0.66      0.64      1539\n",
      "weighted avg       0.67      0.66      0.64      1539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# X_test.shape = (1,data.shape[0],data.shape[1])\n",
    "from sklearn.metrics import classification_report as cr\n",
    "\n",
    "y_prob = model.predict(X_test) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "y_true = y_test.argmax(axis=-1)\n",
    "print(cr(y_true, y_pred , target_names=one_hot_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 2s 19ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.69      0.83      0.75       385\n",
      "        fear       0.58      0.31      0.40       385\n",
      "       happy       0.62      0.54      0.58       385\n",
      "         sad       0.60      0.85      0.70       384\n",
      "\n",
      "    accuracy                           0.63      1539\n",
      "   macro avg       0.62      0.63      0.61      1539\n",
      "weighted avg       0.62      0.63      0.61      1539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##OLD\n",
    "\n",
    "# X_test.shape = (1,data.shape[0],data.shape[1])\n",
    "from sklearn.metrics import classification_report as cr\n",
    "\n",
    "y_prob = model.predict(X_test) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "y_true = y_test.argmax(axis=-1)\n",
    "print(cr(y_true, y_pred , target_names=one_hot_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"Emotion_Detector_new_CNN_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.75      0.88      0.81       385\n",
      "        fear       0.62      0.64      0.63       385\n",
      "       happy       0.71      0.63      0.67       385\n",
      "         sad       0.76      0.69      0.72       384\n",
      "\n",
      "    accuracy                           0.71      1539\n",
      "   macro avg       0.71      0.71      0.71      1539\n",
      "weighted avg       0.71      0.71      0.71      1539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OLD\n",
    "# __________________________________\n",
    "df_test = pd.read_csv(\"Test.csv\")\n",
    "X_test = np.load(\"Test Data - Extracted Features - MFCC_25.npy\")\n",
    "y_test = df_test[\"Emotions\"] \n",
    "\n",
    "one_hot_encoder = preprocessing.LabelBinarizer()\n",
    "one_hot_encoder.fit(pd.read_csv(\"Train-1.csv\")[\"Emotions\"].unique())\n",
    "y_test = one_hot_encoder.transform(y_test)\n",
    "y_prob = model.predict(X_test) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "y_true = y_test.argmax(axis=-1)\n",
    "print(cr(y_true, y_pred , target_names=one_hot_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"Emotion_Detector_CNN_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.87      0.82      0.84       385\n",
      "        fear       0.58      0.73      0.65       385\n",
      "       happy       0.66      0.72      0.69       385\n",
      "         sad       0.84      0.58      0.68       384\n",
      "\n",
      "    accuracy                           0.71      1539\n",
      "   macro avg       0.74      0.71      0.72      1539\n",
      "weighted avg       0.74      0.71      0.72      1539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"Test.csv\")\n",
    "X_test = np.load(\"Test Data - Extracted Features - MFCC_15.npy\")\n",
    "y_test = df_test[\"Emotions\"] \n",
    "\n",
    "one_hot_encoder = preprocessing.LabelBinarizer()\n",
    "one_hot_encoder.fit(pd.read_csv(\"Train-1.csv\")[\"Emotions\"].unique())\n",
    "y_test = one_hot_encoder.transform(y_test)\n",
    "y_prob = model.predict(X_test) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "y_true = y_test.argmax(axis=-1)\n",
    "print(cr(y_true, y_pred , target_names=one_hot_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59064, 303, 45)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
